{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "python_bbvi_ed2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMsw1X7Xs9bheNH6VKe9C+J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kwameasare/mcorp/blob/master/python_bbvi_ed2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJEs1wQHCCxv",
        "colab_type": "code",
        "outputId": "041978d7-5e0b-412c-e1f9-dec0f62cf46a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        }
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "import csv\n",
        "import os\n",
        "import time \n",
        "\n",
        "# Dependency imports\n",
        "from absl import flags\n",
        "# import sys\n",
        "from six.moves import urllib\n",
        "# import edward as ed\n",
        "# from edward.models import Empirical, Gamma, Poisson\n",
        "import tensorflow.compat.v1 as tf1\n",
        "# tf1.disable_v2_behavior()\n",
        "import tensorflow as tf\n",
        "tf.executing_eagerly()\n",
        "tf.enable_v2_behavior()\n",
        "\n",
        "EAGER = False\n",
        "\n",
        "import cProfile\n",
        "\n",
        "import tensorflow_probability as tfp \n",
        "from tensorflow_probability import edward2 as ed\n",
        "dir(ed)\n",
        "import numpy as np\n",
        "\n",
        "# np.set_printoptions(threshold=np.inf)\n",
        "\n",
        "\n",
        "# flags.DEFINE_float(\"learning_rate\",\n",
        "#                    default=1e-4,\n",
        "#                    help=\"Initial learning rate.\")\n",
        "# flags.DEFINE_integer(\"max_steps\",\n",
        "#                      default=200000,\n",
        "#                      help=\"Number of training steps to run.\")\n",
        "# flags.DEFINE_list(\"layer_sizes\",\n",
        "#                   default=[\"100\", \"30\", \"15\"],\n",
        "#                   help=\"Comma-separated list denoting number of latent \"\n",
        "#                        \"variables (stochastic units) per layer.\")\n",
        "# flags.DEFINE_float(\"shape\",\n",
        "#                    default=0.1,\n",
        "#                    help=\"Shape hyperparameter for Gamma priors on latents.\")\n",
        "# flags.DEFINE_string(\"data_dir\",\n",
        "#                     default=os.path.join(os.getenv(\"TEST_TMPDIR\", \"/tmp\"),\n",
        "#                                          \"deep_exponential_family/data\"),\n",
        "#                     help=\"Directory where data is stored (if using real data).\")\n",
        "# flags.DEFINE_string(\"model_dir\",\n",
        "#                     default=os.path.join(os.getenv(\"TEST_TMPDIR\", \"/tmp\"),\n",
        "#                                          \"deep_exponential_family/\"),\n",
        "#                     help=\"Directory to put the model's fit.\")\n",
        "# flags.DEFINE_bool(\"fake_data\",\n",
        "#                   default=None,\n",
        "#                   help=\"If true, uses fake data. Defaults to real data.\")\n",
        "\n",
        "FLAGS = flags.FLAGS\n",
        "\n",
        "\n",
        "bag_of_words = np.random.poisson(5., size=[256, 32000])  # training data as matrix of counts\n",
        "data_size, feature_size = bag_of_words.shape  # number of documents x words (vocabulary)\n",
        "units = [100, 30, 15]  # number of stochastic units per layer\n",
        "shape = 0.1  # Gamma shape parameter\n",
        "\n",
        "w2 = ed.Gamma(0.1, 0.3, sample_shape=[units[2], units[1]])\n",
        "w1 = ed.Gamma(0.1, 0.3, sample_shape=[units[1], units[0]])\n",
        "w0 = ed.Gamma(0.1, 0.3, sample_shape=[units[0], feature_size])\n",
        "\n",
        "z2 = ed.Gamma(0.1, 0.1, sample_shape=[data_size, units[2]])\n",
        "z1 = ed.Gamma(shape, shape / tf.matmul(z2, w2))\n",
        "\n",
        "z0 = ed.Gamma(shape, shape / tf.matmul(z1, w1))\n",
        "x = ed.Gamma(0.1, tf.matmul(z0, w0))\n",
        "\n",
        "print(bag_of_words)\n",
        "\n",
        "\n",
        "def deep_exponential_family(data_size, feature_size, units, shape):\n",
        "  \"\"\"A multi-layered topic model over a documents-by-terms matrix.\"\"\"\n",
        "  w2 = ed.Gamma(0.1, 0.3, sample_shape=[units[2], units[1]], name=\"w2\")\n",
        "  w1 = ed.Gamma(0.1, 0.3, sample_shape=[units[1], units[0]], name=\"w1\")\n",
        "  w0 = ed.Gamma(0.1, 0.3, sample_shape=[units[0], feature_size], name=\"w0\")\n",
        "\n",
        "  z2 = ed.Gamma(0.1, 0.1, sample_shape=[data_size, units[2]], name=\"z2\")\n",
        "  z1 = ed.Gamma(shape, shape / tf.matmul(z2, w2), name=\"z1\")\n",
        "  z0 = ed.Gamma(shape, shape / tf.matmul(z1, w1), name=\"z0\")\n",
        "  x = ed.Poisson(tf.matmul(z0, w0), name=\"x\")\n",
        "  print(x)\n",
        "  return x\n",
        "\n",
        "\n",
        "\n",
        "# Generate from model: returns tf.Tensor of shape (data_size, feature_size).\n",
        "x = deep_exponential_family(data_size = 256 , feature_size = 32000, units = [100, 30, 15], shape = 0.1)\n",
        "# x.numpy()  # converts from eagerly executed tf.Tensor to np.ndarray\n",
        "# with tf.Session() as sess: \n",
        "#   sess.run(x)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[5 8 2 ... 5 5 4]\n",
            " [7 4 4 ... 3 4 5]\n",
            " [7 6 2 ... 7 4 7]\n",
            " ...\n",
            " [2 4 6 ... 2 9 6]\n",
            " [7 6 1 ... 2 6 3]\n",
            " [6 7 4 ... 4 7 4]]\n",
            "RandomVariable(\"\n",
            "[[5.4400e+02 4.5430e+03 1.0573e+04 ... 1.9490e+03 4.7251e+04 1.1225e+04]\n",
            " [7.5000e+01 1.0000e+00 1.9000e+02 ... 6.4000e+01 3.9000e+01 9.9000e+01]\n",
            " [2.3000e+01 4.0000e+01 3.7000e+01 ... 7.3000e+01 1.7900e+02 1.0300e+02]\n",
            " ...\n",
            " [8.0000e+01 9.0000e+00 2.6000e+01 ... 1.6000e+01 1.4800e+02 8.6000e+01]\n",
            " [3.6000e+01 3.8700e+02 7.0440e+04 ... 7.1000e+02 3.0200e+02 7.2545e+04]\n",
            " [1.7210e+03 5.1000e+02 4.8800e+02 ... 2.2600e+02 2.7900e+02 1.6300e+03]]\", shape=(256, 32000), dtype=float32, device=/job:localhost/replica:0/task:0/device:CPU:0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_jhUQ9dnoL5",
        "colab_type": "code",
        "outputId": "5bed10f4-81ef-489c-9da3-fb1b2dc2e6ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "def trainable_positive_deterministic(shape, min_loc=1e-3, name=None):\n",
        "  \"\"\"Learnable Deterministic distribution over positive reals.\"\"\"\n",
        "  with tf.compat.v1.variable_scope(\n",
        "      None, default_name=\"trainable_positive_deterministic\"):\n",
        "    unconstrained_loc = tf.compat.v1.get_variable(\"unconstrained_loc\", shape)\n",
        "    loc = tf.maximum(tf.nn.softplus(unconstrained_loc), min_loc)\n",
        "    rv = ed.Deterministic(loc=loc, name=name)\n",
        "\n",
        "    print(rv)\n",
        "    return rv\n",
        "\n",
        "def trainable_gamma(shape, min_concentration=1e-3, min_scale=1e-5, name=None):\n",
        "  \"\"\"Learnable Gamma via concentration and scale parameterization.\"\"\"\n",
        "  with tf.compat.v1.variable_scope(None, default_name=\"trainable_gamma\"):\n",
        "    unconstrained_concentration = tf.compat.v1.get_variable(\n",
        "        \"unconstrained_concentration\",\n",
        "        shape,\n",
        "        initializer=tf.compat.v1.initializers.random_normal(\n",
        "            mean=0.5, stddev=0.1))\n",
        "    unconstrained_scale = tf.compat.v1.get_variable(\n",
        "        \"unconstrained_scale\",\n",
        "        shape,\n",
        "        initializer=tf.compat.v1.initializers.random_normal(stddev=0.1))\n",
        "    concentration = tf.maximum(tf.nn.softplus(unconstrained_concentration),\n",
        "                               min_concentration)\n",
        "    rate = tf.maximum(1. / tf.nn.softplus(unconstrained_scale), 1. / min_scale)\n",
        "    rv = ed.Gamma(concentration=concentration, rate=rate, name=name)\n",
        "    print(rv)\n",
        "    return rv\n",
        "\n",
        "\n",
        "\n",
        "eg = trainable_positive_deterministic(shape = (3, 6), min_loc=1e-3, name = \"Dave\")\n",
        "eg2 = trainable_gamma(shape = (3, 6), min_concentration=1e-3, min_scale=1e-3, name = \"Dave2\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RandomVariable(\"\n",
            "[[0.4286835  0.4863159  0.76186407 0.9298632  0.53964955 0.44855154]\n",
            " [0.5355686  0.9443109  0.57048243 0.77937424 0.66257924 0.4894936 ]\n",
            " [0.78970104 0.54046637 0.9839853  0.5362346  0.5936479  0.38427094]]\", shape=(3, 6), dtype=float32, device=/job:localhost/replica:0/task:0/device:CPU:0)\n",
            "RandomVariable(\"\n",
            "[[2.1674184e-03 5.6429963e-05 8.8354101e-04 6.9050246e-04 6.5078365e-04\n",
            "  2.8824492e-04]\n",
            " [6.1750755e-04 5.3683505e-04 9.4312170e-05 2.5020866e-04 7.7176787e-04\n",
            "  8.6535246e-04]\n",
            " [1.8631553e-04 1.2163562e-04 1.0036634e-03 2.3673593e-04 1.6675701e-03\n",
            "  6.9796115e-05]]\", shape=(3, 6), dtype=float32, device=/job:localhost/replica:0/task:0/device:CPU:0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIsH9ITRzd4l",
        "colab_type": "code",
        "outputId": "6c33d52b-1dc5-4ccd-a952-447957bb6c52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "bag_of_words = np.random.poisson(5., size=[256, 32000])  # training data as matrix of counts\n",
        "data_size, feature_size = bag_of_words.shape  # number of documents x words (vocabulary)\n",
        "units = [100, 30, 15]  # number of stochastic units per layer\n",
        "shape = 0.1  # Gamma shape parameter\n",
        "\n",
        "print(bag_of_words.shape)\n",
        "\n",
        "def deep_exponential_family_variational(data_size, feature_size, units):\n",
        "  \"\"\"Posterior approx. for deep exponential family p(w{0,1,2}, z{1,2,3} | x).\"\"\"\n",
        "  qw2 = trainable_positive_deterministic([units[2], units[1]], name=\"qw2\")\n",
        "  qw1 = trainable_positive_deterministic([units[1], units[0]], name=\"qw1\")\n",
        "  qw0 = trainable_positive_deterministic([units[0], feature_size], name=\"qw0\")\n",
        "  qz2 = trainable_gamma([data_size, units[2]], name=\"qz2\")\n",
        "  qz1 = trainable_gamma([data_size, units[1]], name=\"qz1\")\n",
        "  qz0 = trainable_gamma([data_size, units[0]], name=\"qz0\")\n",
        "  \n",
        "  return qw2, qw1, qw0, qz2, qz1, qz0\n",
        "\n",
        "\n",
        "eg = deep_exponential_family_variational(data_size = (256, 32000), feature_size = (256, 32000), units = [100, 30, 15])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(256, 32000)\n",
            "RandomVariable(\"\n",
            "[[0.6255446  0.8114654  0.7891374  0.8886018  0.5394876  0.8364475\n",
            "  0.8828017  0.71148735 0.55205303 0.57565504 0.73650116 0.5495552\n",
            "  0.76067954 0.75581944 0.6400035  0.62803847 0.5910707  0.7573783\n",
            "  0.560536   0.67564374 0.6093487  0.83882165 0.5412694  0.74518883\n",
            "  0.7932688  0.7861298  0.539871   0.64821166 0.7191437  0.8682095 ]\n",
            " [0.66428435 0.7965848  0.5892857  0.82839143 0.59894127 0.7028111\n",
            "  0.55705374 0.55072    0.8576871  0.69677234 0.8144127  0.7283403\n",
            "  0.7874715  0.61951053 0.7539011  0.5319842  0.7861264  0.8289107\n",
            "  0.5336695  0.5992313  0.8432434  0.53485703 0.8579883  0.56902957\n",
            "  0.72489285 0.76901084 0.6550898  0.64060795 0.5540998  0.754682  ]\n",
            " [0.6768164  0.7588457  0.63601846 0.72876585 0.7111478  0.6830122\n",
            "  0.6745105  0.55184966 0.7736014  0.6674448  0.60791355 0.6885026\n",
            "  0.72230566 0.8266524  0.7298543  0.64486265 0.7808952  0.65651625\n",
            "  0.79968935 0.6961094  0.73536354 0.8331728  0.58277434 0.69913733\n",
            "  0.56186104 0.81925327 0.71156263 0.5328256  0.5764726  0.60187376]\n",
            " [0.6562728  0.676413   0.8326658  0.6259638  0.5720271  0.5507694\n",
            "  0.57660955 0.69128984 0.7650659  0.60397553 0.813844   0.6449031\n",
            "  0.78028196 0.85385185 0.8004519  0.7057136  0.8319471  0.5831183\n",
            "  0.7642188  0.8554582  0.6805767  0.53455037 0.757423   0.5277861\n",
            "  0.73315066 0.7202427  0.6237005  0.67430896 0.64132226 0.61165047]\n",
            " [0.6652823  0.81783783 0.77348095 0.7567142  0.8088489  0.7443143\n",
            "  0.8014724  0.62699103 0.53392845 0.7682514  0.6576701  0.8649904\n",
            "  0.88183975 0.7069374  0.6794594  0.7750952  0.59358394 0.72279\n",
            "  0.6493615  0.6234212  0.6919414  0.8586819  0.6325718  0.55148554\n",
            "  0.5406444  0.56597424 0.8126898  0.71972686 0.8040625  0.6782889 ]\n",
            " [0.69610536 0.77091557 0.7583169  0.5301763  0.6024683  0.7863483\n",
            "  0.66850275 0.573494   0.7285092  0.6000104  0.8693112  0.6453014\n",
            "  0.5555638  0.84854734 0.8240221  0.73613876 0.6184684  0.7330291\n",
            "  0.5480618  0.6205057  0.53446615 0.58219415 0.7372564  0.54932\n",
            "  0.7197619  0.667703   0.5621433  0.6516675  0.7741309  0.5800854 ]\n",
            " [0.81718254 0.5689901  0.54381955 0.6183283  0.86899793 0.55569077\n",
            "  0.86733043 0.62452555 0.8458802  0.6234835  0.6348659  0.566741\n",
            "  0.58976054 0.7498122  0.77017206 0.74281985 0.5404904  0.62103707\n",
            "  0.5986614  0.5929495  0.86102134 0.76036036 0.8602239  0.642716\n",
            "  0.7212246  0.5460249  0.8751518  0.63562423 0.6444981  0.60255873]\n",
            " [0.7189116  0.8836454  0.6131033  0.8473351  0.6701559  0.5534012\n",
            "  0.57007843 0.65576243 0.6679307  0.6773768  0.8569378  0.6937947\n",
            "  0.80168796 0.5566464  0.6277694  0.7411593  0.6021216  0.84248126\n",
            "  0.6860026  0.620368   0.5754317  0.8106413  0.6585504  0.6853261\n",
            "  0.7492818  0.88559973 0.7488218  0.74750584 0.5322275  0.74888694]\n",
            " [0.84875995 0.7926632  0.8501884  0.81323606 0.55504113 0.77156764\n",
            "  0.5380181  0.62276316 0.7924734  0.6133364  0.63151926 0.73700786\n",
            "  0.78503096 0.6802127  0.7703889  0.5332756  0.6365183  0.7831087\n",
            "  0.6943781  0.85840625 0.7462485  0.8696861  0.675084   0.87071145\n",
            "  0.5943372  0.8059656  0.6054905  0.58300436 0.57879275 0.55870855]\n",
            " [0.7905283  0.70346946 0.6475451  0.5977696  0.8753088  0.6882954\n",
            "  0.5523437  0.66341746 0.5522598  0.6042797  0.5969392  0.68610185\n",
            "  0.59163904 0.8792161  0.63119006 0.58904326 0.6856077  0.52719665\n",
            "  0.64408493 0.85976344 0.52731407 0.52918917 0.72068965 0.7559376\n",
            "  0.56541175 0.7896913  0.5716743  0.53505474 0.7668914  0.81283915]\n",
            " [0.7349351  0.8325621  0.55717456 0.6975604  0.67503613 0.5701304\n",
            "  0.616735   0.8530366  0.54042745 0.64014804 0.5765103  0.70672715\n",
            "  0.6436214  0.7762112  0.88347924 0.564362   0.82731324 0.63347656\n",
            "  0.739249   0.7384673  0.57541156 0.64208543 0.8343426  0.6696172\n",
            "  0.54653037 0.7638483  0.72802716 0.75515604 0.5459697  0.8626221 ]\n",
            " [0.78063065 0.6133548  0.61036384 0.7104988  0.62700623 0.54199433\n",
            "  0.8771128  0.7765134  0.70691365 0.5586365  0.53202057 0.765937\n",
            "  0.8463905  0.6915836  0.8414388  0.8744528  0.72626317 0.7347898\n",
            "  0.87054425 0.5888936  0.81833136 0.8384558  0.7216392  0.78774315\n",
            "  0.5618634  0.8425917  0.7516886  0.6868561  0.72348666 0.66370934]\n",
            " [0.7141022  0.85645396 0.5829993  0.8260807  0.8190532  0.5334567\n",
            "  0.6389834  0.798046   0.60295624 0.60590404 0.7686728  0.73654085\n",
            "  0.8828858  0.82266545 0.64223784 0.7273985  0.5467848  0.56791097\n",
            "  0.7715697  0.6561407  0.81524837 0.8172376  0.7056272  0.7803124\n",
            "  0.834954   0.6139001  0.6052392  0.5990513  0.6046717  0.86763334]\n",
            " [0.69259137 0.7776457  0.71639067 0.5649756  0.5993234  0.74219155\n",
            "  0.62822926 0.7743294  0.6109368  0.69996506 0.56153584 0.57404435\n",
            "  0.76519483 0.7891609  0.8166524  0.6327209  0.70405966 0.84006983\n",
            "  0.71950835 0.6643688  0.62830013 0.7228226  0.62324417 0.53993905\n",
            "  0.6130758  0.53806007 0.79305166 0.86110914 0.6896918  0.68754077]\n",
            " [0.65077055 0.63370824 0.78666973 0.5750742  0.67757237 0.74228513\n",
            "  0.8391565  0.7018636  0.8355621  0.6026207  0.5762398  0.858863\n",
            "  0.8328935  0.8413329  0.73744154 0.5319295  0.802876   0.8510456\n",
            "  0.82242864 0.71845865 0.8495446  0.62045515 0.6003077  0.88852787\n",
            "  0.74504125 0.656708   0.55768    0.7058146  0.7671503  0.77350205]]\", shape=(15, 30), dtype=float32, device=/job:localhost/replica:0/task:0/device:CPU:0)\n",
            "RandomVariable(\"\n",
            "[[0.59551644 0.7908665  0.5940704  ... 0.78512394 0.7319727  0.6467626 ]\n",
            " [0.66746265 0.74795073 0.6218507  ... 0.7890757  0.70623523 0.6406875 ]\n",
            " [0.68848634 0.77411216 0.7373608  ... 0.763815   0.7046146  0.7568774 ]\n",
            " ...\n",
            " [0.7688256  0.6007033  0.6890551  ... 0.6832546  0.7663034  0.7275692 ]\n",
            " [0.7038078  0.65348035 0.72606504 ... 0.76353604 0.6243703  0.6162011 ]\n",
            " [0.7843875  0.8030365  0.7328196  ... 0.7256298  0.660499   0.6123821 ]]\", shape=(30, 100), dtype=float32, device=/job:localhost/replica:0/task:0/device:CPU:0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-78-cb0680cc7a39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0meg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeep_exponential_family_variational\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-78-cb0680cc7a39>\u001b[0m in \u001b[0;36mdeep_exponential_family_variational\u001b[0;34m(data_size, feature_size, units)\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0mqw2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainable_positive_deterministic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"qw2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0mqw1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainable_positive_deterministic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"qw1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m   \u001b[0mqw0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainable_positive_deterministic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"qw0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m   \u001b[0mqz2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainable_gamma\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"qz2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0mqz1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainable_gamma\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"qz1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-77-c499fb347dc4>\u001b[0m in \u001b[0;36mtrainable_positive_deterministic\u001b[0;34m(shape, min_loc, name)\u001b[0m\n\u001b[1;32m      3\u001b[0m   with tf.compat.v1.variable_scope(\n\u001b[1;32m      4\u001b[0m       None, default_name=\"trainable_positive_deterministic\"):\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0munconstrained_loc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"unconstrained_loc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftplus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munconstrained_loc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_loc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mrv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0med\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDeterministic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m   1498\u001b[0m       \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1499\u001b[0m       \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1500\u001b[0;31m       aggregation=aggregation)\n\u001b[0m\u001b[1;32m   1501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m   1241\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1242\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1243\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m   1244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1245\u001b[0m   def _get_partitioned_variable(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    565\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 567\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m    568\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m   def _get_partitioned_variable(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    517\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m     synchronization, aggregation, trainable = (\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    848\u001b[0m     \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 849\u001b[0;31m     \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    851\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36mas_shape\u001b[0;34m(shape)\u001b[0m\n\u001b[1;32m   1214\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mTensorShape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dims)\u001b[0m\n\u001b[1;32m    774\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0;31m# Got a list of dimensions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 776\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mas_dimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdims_iter\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    777\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    774\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0;31m# Got a list of dimensions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 776\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mas_dimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdims_iter\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    777\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36mas_dimension\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m    716\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 718\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mDimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    719\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    191\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot convert %s to Dimension\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m       if (not isinstance(value, compat.bytes_or_text_types) and\n\u001b[1;32m    195\u001b[0m           self._value != value):\n",
            "\u001b[0;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a number, not 'tuple'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWvChaSL1zSy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_nips2011_papers(path):\n",
        "  \"\"\"Loads NIPS 2011 conference papers.\n",
        "  The NIPS 1987-2015 data set is in the form of a 11,463 x 5,812 matrix of\n",
        "  per-paper word counts, containing 11,463 words and 5,811 NIPS conference\n",
        "  papers (Perrone et al., 2016). We subset to papers in 2011 and words appearing\n",
        "  in at least two documents and having a total word count of at least 10.\n",
        "  Built from the Observations Python package.\n",
        "  Args:\n",
        "    path: str.\n",
        "      Path to directory which either stores file or otherwise file will\n",
        "      be downloaded and extracted there. Filename is `NIPS_1987-2015.csv`.\n",
        "  Returns:\n",
        "    bag_of_words: np.ndarray of shape [num_documents, num_words]. Each element\n",
        "      denotes the number of occurrences of a specific word in a specific\n",
        "      document.\n",
        "    words: List of strings, denoting the words for `bag_of_words`'s columns.\n",
        "  \"\"\"\n",
        "  path = os.path.expanduser(path)\n",
        "  filename = \"NIPS_1987-2015.csv\"\n",
        "  filepath = os.path.join(path, filename)\n",
        "  if not os.path.exists(filepath):\n",
        "    url = (\"https://archive.ics.uci.edu/ml/machine-learning-databases/\"\n",
        "           \"00371/NIPS_1987-2015.csv\")\n",
        "    if not tf.io.gfile.exists(path):\n",
        "      tf.io.gfile.makedirs(path)\n",
        "    print(\"Downloading %s to %s\" % (url, filepath))\n",
        "    urllib.request.urlretrieve(url, filepath)\n",
        "\n",
        "  with open(filepath) as f:\n",
        "    iterator = csv.reader(f)\n",
        "    documents = next(iterator)[1:]\n",
        "    words = []\n",
        "    x_train = []\n",
        "    for row in iterator:\n",
        "      words.append(row[0])\n",
        "      x_train.append(row[1:])\n",
        "\n",
        "  x_train = np.array(x_train, dtype=np.int)\n",
        "\n",
        "  # Subset to documents in 2011 and words appearing in at least two documents\n",
        "  # and have a total word count of at least 10.\n",
        "  doc_idx = [i for i, document in enumerate(documents)\n",
        "             if document.startswith(\"2011\")]\n",
        "  documents = [documents[doc] for doc in doc_idx]\n",
        "  x_train = x_train[:, doc_idx]\n",
        "  word_idx = np.logical_and(np.sum(x_train != 0, 1) >= 2,\n",
        "                            np.sum(x_train, 1) >= 10)\n",
        "  words = [word for word, idx in zip(words, word_idx) if idx]\n",
        "  bag_of_words = x_train[word_idx, :].T\n",
        "  print(bag_of_words)\n",
        "  print(words)\n",
        "  \n",
        "  print(x_train)\n",
        "  \n",
        "  return bag_of_words, words\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWAzb-U-agQ_",
        "colab_type": "code",
        "outputId": "ea570fd8-5a7b-48f7-cb01-6cbe258287c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        }
      },
      "source": [
        "eg = load_nips2011_papers(\"tmp\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 3]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "['abc', 'abernethy', 'abilities', 'ability', 'ables', 'abs', 'absence', 'absent', 'absolute', 'abstract', 'abstraction', 'abstractions', 'abuse', 'academic', 'academy', 'accelerate', 'accelerated', 'acceleration', 'accept', 'acceptance', 'accepted', 'accepting', 'access', 'accesses', 'accommodate', 'accomplish', 'accomplished', 'accord', 'accordance', 'according', 'accordingly', 'account', 'accounted', 'accounting', 'accounts', 'accumulated', 'accumulation', 'accuracies', 'accuracy', 'accurate', 'accurately', 'achievable', 'achieve', 'achieved', 'achieves', 'achieving', 'acknowledge', 'acknowledgement', 'acknowledgements', 'acknowledges', 'acknowledgments', 'acl', 'acm', 'acoustics', 'acquire', 'acquired', 'acquisition', 'across', 'act', 'acting', 'action', 'actions', 'activated', 'activation', 'activations', 'active', 'actively', 'activities', 'activity', 'actor', 'acts', 'actual', 'actually', 'acyclic', 'adaboost', 'adam', 'adams', 'adapt', 'adaptation', 'adapted', 'adapting', 'adaptive', 'adaptively', 'adapts', 'add', 'added', 'adding', 'addition', 'additional', 'additionally', 'additive', 'address', 'addressed', 'addresses', 'addressing', 'adds', 'adequate', 'adjacency', 'adjacent', 'adjoint', 'adjust', 'adjusted', 'adjusting', 'adjustment', 'admissible', 'admit', 'admits', 'adopt', 'adopted', 'adopting', 'ads', 'adult', 'advance', 'advanced', 'advances', 'advantage', 'advantageous', 'advantages', 'adversarial', 'adversaries', 'adversary', 'advertising', 'advice', 'affect', 'affected', 'affecting', 'affects', 'affine', 'affinities', 'affinity', 'aforementioned', 'afosr', 'agarwal', 'age', 'agency', 'agent', 'agents', 'agglomerative', 'aggregate', 'aggregated', 'aggregating', 'aggregation', 'aggressive', 'agnostic', 'agree', 'agreement', 'agrees', 'ahead', 'aid', 'aim', 'aimed', 'aims', 'ais', 'aistats', 'alan', 'alarm', 'albeit', 'alberta', 'alessandro', 'alex', 'alexander', 'alexandre', 'alg', 'algebra', 'algebraic', 'algo', 'algorithm', 'algorithmic', 'algorithms', 'ali', 'align', 'aligned', 'alignment', 'alleviate', 'allocated', 'allocation', 'allow', 'allowed', 'allowing', 'allows', 'ally', 'alm', 'almost', 'alone', 'along', 'alpha', 'alphabet', 'already', 'als', 'also', 'alter', 'altered', 'alternate', 'alternating', 'alternative', 'alternatively', 'alternatives', 'although', 'altun', 'always', 'alzheimers', 'amari', 'amazon', 'ambient', 'ambiguity', 'ambiguous', 'amenable', 'amer', 'america', 'american', 'amherst', 'amir', 'among', 'amongst', 'amount', 'amounts', 'amplitude', 'amplitudes', 'ams', 'amsterdam', 'anal', 'analog', 'analogous', 'analogously', 'analogue', 'analogy', 'analyses', 'analysis', 'analytic', 'analytical', 'analytically', 'analyze', 'analyzed', 'analyzing', 'anatomical', 'ance', 'ancestors', 'anchor', 'anderson', 'ando', 'andor', 'andreas', 'andrew', 'angeles', 'angle', 'angular', 'animal', 'animals', 'anisotropic', 'ann', 'annals', 'annealing', 'annotate', 'annotated', 'annotation', 'annotations', 'annotator', 'annotators', 'annual', 'anomalies', 'anomalous', 'anomaly', 'anonymous', 'another', 'anova', 'answer', 'answered', 'answers', 'anti', 'anything', 'apart', 'apparent', 'apparently', 'appealing', 'appear', 'appearance', 'appeared', 'appearing', 'appears', 'appendix', 'appl', 'appli', 'applica', 'applicability', 'applicable', 'application', 'applications', 'applied', 'applies', 'apply', 'applying', 'apprenticeship', 'approach', 'approaches', 'approaching', 'appropriate', 'appropriately', 'approx', 'approxi', 'approxima', 'approximate', 'approximated', 'approximately', 'approximates', 'approximating', 'approximation', 'approximations', 'approximator', 'april', 'arbitrarily', 'arbitrary', 'arc', 'architecture', 'architectures', 'arcs', 'ard', 'area', 'areas', 'arg', 'argmax', 'argmin', 'arguably', 'argue', 'argued', 'argument', 'arguments', 'argyriou', 'arise', 'arises', 'arising', 'arithmetic', 'arizona', 'arm', 'armed', 'arms', 'arora', 'around', 'arranged', 'arrangement', 'array', 'arrays', 'arrive', 'arrow', 'arrows', 'art', 'article', 'articles', 'artifacts', 'artificial', 'artificially', 'arxivv', 'ary', 'ascent', 'aside', 'asked', 'asking', 'aspect', 'aspects', 'assess', 'assessed', 'assessing', 'assessment', 'assign', 'assigned', 'assigning', 'assignment', 'assignments', 'assigns', 'assoc', 'associate', 'associated', 'associating', 'association', 'associations', 'associative', 'assume', 'assumed', 'assumes', 'assuming', 'assump', 'assumption', 'assumptions', 'asuncion', 'asymmetric', 'asymmetry', 'asymptotic', 'asymptotically', 'asynchronous', 'ate', 'athena', 'ation', 'atlas', 'atom', 'atomic', 'atoms', 'attain', 'attainable', 'attained', 'attains', 'attempt', 'attempted', 'attempting', 'attempts', 'attention', 'attracted', 'attractive', 'attribute', 'attributed', 'attributes', 'audibert', 'audio', 'auditory', 'aug', 'augment', 'augmented', 'august', 'austin', 'australian', 'auto', 'autoencoder', 'autoencoders', 'automated', 'automatic', 'automatically', 'autonomous', 'autoregressive', 'auxiliary', 'availability', 'available', 'ave', 'avenue', 'average', 'averaged', 'averages', 'averaging', 'avoid', 'avoided', 'avoiding', 'avoids', 'award', 'aware', 'away', 'axes', 'axis', 'bach', 'back', 'background', 'backgrounds', 'backpropagation', 'backup', 'backward', 'bad', 'badly', 'bag', 'bags', 'balance', 'balanced', 'balances', 'balancing', 'balcan', 'ball', 'balls', 'banach', 'band', 'bandit', 'bandits', 'bandwidth', 'bank', 'bar', 'baraniuk', 'barber', 'bars', 'bartlett', 'barto', 'base', 'based', 'baseline', 'baselines', 'bases', 'basic', 'basically', 'basis', 'bat', 'batch', 'batches', 'battle', 'baum', 'bayes', 'bayesian', 'beal', 'bear', 'became', 'beck', 'become', 'becomes', 'beforehand', 'begin', 'beginning', 'begins', 'behav', 'behave', 'behaved', 'behaves', 'behavior', 'behavioral', 'behaviors', 'behaviour', 'behind', 'belief', 'beliefs', 'believe', 'believed', 'belkin', 'bell', 'bellman', 'belong', 'belongie', 'belonging', 'belongs', 'ben', 'benchmark', 'benchmarks', 'beneficial', 'benefit', 'benefits', 'bengio', 'benign', 'bennett', 'ber', 'berg', 'berger', 'berkeley', 'berlin', 'bernoulli', 'bernstein', 'bertsekas', 'besides', 'best', 'beta', 'bethe', 'bethge', 'better', 'beygelzimer', 'bfgs', 'bialek', 'bianchi', 'bias', 'biased', 'biases', 'bic', 'bickel', 'bicycle', 'big', 'bigger', 'bilinear', 'bility', 'bilmes', 'bin', 'binary', 'binding', 'binocular', 'binomial', 'bins', 'bioinformatics', 'biol', 'biological', 'biologically', 'biology', 'biomedical', 'biometrika', 'biophysical', 'bipartite', 'bird', 'birds', 'birth', 'bishop', 'bit', 'bits', 'bivariate', 'black', 'blei', 'bleu', 'blind', 'blitzer', 'block', 'blockmodel', 'blocks', 'blog', 'blogs', 'blood', 'blue', 'blum', 'blurring', 'body', 'bold', 'boltzmann', 'book', 'books', 'boolean', 'boost', 'boosted', 'booster', 'boosting', 'bootstrap', 'border', 'borel', 'borgwardt', 'borrow', 'borrowed', 'borrowing', 'boston', 'bottleneck', 'bottom', 'bottou', 'bound', 'boundaries', 'boundary', 'bounded', 'bounding', 'bounds', 'bousquet', 'bow', 'bowling', 'boxes', 'boyd', 'brain', 'branch', 'branches', 'branching', 'break', 'breaking', 'breast', 'bregman', 'brevity', 'bridge', 'brief', 'briefly', 'bring', 'broad', 'broader', 'broadly', 'broken', 'brought', 'brown', 'brownian', 'brute', 'bubeck', 'buckets', 'budget', 'buffet', 'build', 'building', 'builds', 'built', 'bundle', 'burn', 'bus', 'business', 'bution', 'butions', 'caching', 'calculate', 'calculated', 'calculating', 'calculation', 'calculations', 'calibrated', 'calibration', 'california', 'call', 'called', 'calls', 'cally', 'caltech', 'cam', 'cambridge', 'camera', 'campbell', 'canada', 'cancer', 'candes', 'candidate', 'candidates', 'canonical', 'cant', 'cap', 'capabilities', 'capable', 'capacity', 'caption', 'captions', 'capture', 'captured', 'captures', 'capturing', 'car', 'card', 'cardinality', 'cards', 'care', 'career', 'careful', 'carefully', 'carin', 'carl', 'carlo', 'carnegie', 'carreira', 'carried', 'carrier', 'carries', 'carry', 'carrying', 'cars', 'cart', 'cascade', 'cascaded', 'case', 'cases', 'cast', 'cat', 'categorical', 'categories', 'categorization', 'categorize', 'categorized', 'category', 'cation', 'cations', 'cauchy', 'causal', 'cause', 'caused', 'causes', 'ceedings', 'cell', 'cells', 'center', 'centered', 'centers', 'central', 'centralized', 'centre', 'centroid', 'cerebral', 'certain', 'certainly', 'cesa', 'cess', 'cessing', 'chain', 'chains', 'chair', 'challenge', 'challenges', 'challenging', 'chance', 'chang', 'change', 'changed', 'changes', 'changing', 'channel', 'channels', 'chapelle', 'chapman', 'chapter', 'character', 'characteristic', 'characteristics', 'characterization', 'characterize', 'characterized', 'characterizes', 'characterizing', 'characters', 'charge', 'charles', 'chart', 'cheap', 'cheaper', 'check', 'checked', 'chemical', 'chemistry', 'cheng', 'chicago', 'chichilnisky', 'child', 'children', 'china', 'chinese', 'choice', 'choices', 'cholesky', 'choose', 'chooses', 'choosing', 'chose', 'chosen', 'chow', 'christian', 'christopher', 'chu', 'chung', 'chunking', 'church', 'cient', 'cifar', 'circle', 'circles', 'circuit', 'circuitry', 'circuits', 'circular', 'circumstances', 'cis', 'city', 'claim', 'claims', 'clarity', 'clas', 'class', 'classes', 'classi', 'classic', 'classical', 'classifi', 'classifica', 'classification', 'classified', 'classifier', 'classifiers', 'classify', 'classifying', 'clause', 'clauses', 'clean', 'clear', 'clearly', 'click', 'clicked', 'clicks', 'clinical', 'clique', 'cliques', 'close', 'closed', 'closely', 'closer', 'closest', 'closure', 'cloud', 'clouds', 'clus', 'cluster', 'clustered', 'clustering', 'clusterings', 'clusters', 'clutter', 'cluttered', 'coarse', 'coates', 'coda', 'code', 'codebook', 'coded', 'codes', 'codewords', 'coding', 'coefficient', 'coefficients', 'cog', 'cognition', 'cognitive', 'cohen', 'coherence', 'coherent', 'coin', 'coincide', 'coincides', 'col', 'collaborative', 'collapsed', 'collect', 'collected', 'collecting', 'collection', 'collections', 'collective', 'college', 'collins', 'collision', 'collobert', 'color', 'colored', 'colors', 'colt', 'columbia', 'column', 'columns', 'combination', 'combinations', 'combinatorial', 'combine', 'combined', 'combines', 'combining', 'come', 'comes', 'coming', 'comment', 'comments', 'commit', 'common', 'commonly', 'communicate', 'communication', 'communications', 'communities', 'community', 'comp', 'compact', 'compactness', 'comparable', 'comparably', 'comparative', 'compare', 'compared', 'compares', 'comparing', 'comparison', 'compatibility', 'compatible', 'compensate', 'compete', 'competing', 'competition', 'competitive', 'competitors', 'compilation', 'complement', 'complementary', 'complete', 'completed', 'completely', 'completeness', 'completion', 'complex', 'complexities', 'complexity', 'complicated', 'component', 'components', 'composed', 'composing', 'composite', 'composition', 'compositional', 'compound', 'comprehensive', 'compress', 'compressed', 'compression', 'compressive', 'comprised', 'comprises', 'comprising', 'compu', 'comput', 'computa', 'computable', 'computation', 'computational', 'computationally', 'computations', 'compute', 'computed', 'computer', 'computers', 'computes', 'computing', 'con', 'concatenated', 'concatenating', 'concatenation', 'concave', 'concavity', 'concentrate', 'concentrated', 'concentrates', 'concentration', 'concept', 'concepts', 'conceptual', 'conceptually', 'concern', 'concerned', 'concerning', 'concerns', 'conclude', 'concludes', 'concluding', 'conclusion', 'conclusions', 'concrete', 'concretely', 'concurrent', 'condi', 'condition', 'conditional', 'conditionally', 'conditionals', 'conditioned', 'conditioning', 'conditions', 'conduct', 'conductance', 'conductances', 'conducted', 'cone', 'conf', 'confer', 'conference', 'confidence', 'confident', 'configuration', 'configurations', 'confirm', 'confirmed', 'confirms', 'conflict', 'conflicting', 'confounders', 'confounding', 'confusion', 'conjecture', 'conjugacy', 'conjugate', 'conjunction', 'conjunctions', 'connect', 'connected', 'connecting', 'connection', 'connections', 'connectivity', 'connects', 'conquer', 'consecutive', 'consensus', 'consequence', 'consequences', 'consequently', 'conservative', 'consid', 'consider', 'considerable', 'considerably', 'consideration', 'considerations', 'considered', 'considering', 'considers', 'consist', 'consisted', 'consistency', 'consistent', 'consistently', 'consisting', 'consists', 'const', 'constant', 'constants', 'constitute', 'constitutes', 'constrain', 'constrained', 'constraining', 'constraint', 'constraints', 'construct', 'constructed', 'constructing', 'construction', 'constructions', 'constructive', 'constructs', 'consuming', 'contact', 'contain', 'contained', 'containing', 'contains', 'content', 'context', 'contexts', 'contextual', 'contiguous', 'contingency', 'continue', 'continues', 'continuity', 'continuous', 'continuously', 'continuum', 'contour', 'contours', 'contract', 'contraction', 'contractive', 'contradiction', 'contradicts', 'contrary', 'contrast', 'contrastive', 'contribute', 'contributes', 'contribution', 'contributions', 'control', 'controlled', 'controlling', 'controls', 'conv', 'convenience', 'convenient', 'convention', 'conventional', 'converge', 'converged', 'convergence', 'convergent', 'converges', 'converging', 'conversely', 'conversion', 'convert', 'converted', 'convex', 'convexity', 'convey', 'convolution', 'convolutional', 'cooperative', 'coordinate', 'coordinates', 'cope', 'copies', 'copy', 'cor', 'core', 'coresets', 'cornell', 'corner', 'corollary', 'corpora', 'corpus', 'corr', 'corre', 'correct', 'corrected', 'correcting', 'correction', 'correctly', 'correctness', 'correlate', 'correlated', 'correlates', 'correlation', 'correlations', 'correspond', 'correspondence', 'corresponding', 'corresponds', 'corrupted', 'corruption', 'cortes', 'cortex', 'cortical', 'cortices', 'cosamp', 'cosine', 'cost', 'costly', 'costs', 'council', 'count', 'counter', 'counterpart', 'counterparts', 'counting', 'country', 'counts', 'county', 'couple', 'coupled', 'coupling', 'course', 'covariance', 'covariances', 'covariate', 'covariates', 'cover', 'coverage', 'covered', 'covering', 'covers', 'cpu', 'crammer', 'create', 'created', 'creates', 'creating', 'cristianini', 'criteria', 'criterion', 'critic', 'critical', 'critically', 'cross', 'crosses', 'crowd', 'crowdsourced', 'crowdsourcing', 'crp', 'crucial', 'crucially', 'csaba', 'cube', 'cubic', 'cue', 'cues', 'culotta', 'cumulative', 'cunningham', 'current', 'currently', 'currents', 'curriculum', 'curse', 'cursor', 'curvature', 'curve', 'curves', 'customer', 'customers', 'cut', 'cuts', 'cutting', 'cyan', 'cybernetics', 'cycle', 'cycles', 'cyclic', 'dai', 'dalal', 'damping', 'dan', 'dani', 'daniel', 'dantzig', 'dard', 'dark', 'darpa', 'darrell', 'das', 'dasgupta', 'dashed', 'daspremont', 'data', 'database', 'databases', 'datapoints', 'dataset', 'datasets', 'date', 'dates', 'daume', 'david', 'davis', 'day', 'dayan', 'days', 'deal', 'dealing', 'deals', 'dealt', 'death', 'dec', 'decade', 'decay', 'decays', 'december', 'decentralized', 'decide', 'decided', 'deciding', 'decision', 'decisions', 'decode', 'decoded', 'decoder', 'decoding', 'decomposable', 'decompose', 'decomposed', 'decomposes', 'decomposing', 'decomposition', 'decompositions', 'decoupled', 'decoupling', 'decrease', 'decreased', 'decreases', 'decreasing', 'deduce', 'deemed', 'deep', 'deeper', 'default', 'defense', 'defer', 'define', 'defined', 'defines', 'defining', 'definite', 'definition', 'definitions', 'deformable', 'deformation', 'deg', 'degeneracy', 'degenerate', 'degradation', 'degrade', 'degrades', 'degree', 'degrees', 'dekel', 'delay', 'delayed', 'delays', 'delete', 'deletion', 'deliver', 'delta', 'demand', 'demixing', 'demodulation', 'demon', 'demonstrates', 'demonstrating', 'demonstration', 'demonstrations', 'den', 'dence', 'dendrites', 'dendritic', 'deng', 'denoising', 'denominator', 'denote', 'denoted', 'denotes', 'denoting', 'dense', 'densely', 'densities', 'density', 'dent', 'department', 'depen', 'depend', 'dependence', 'dependencies', 'dependency', 'dependent', 'depending', 'depends', 'depicted', 'depicts', 'depression', 'dept', 'depth', 'derivation', 'derivations', 'derivative', 'derivatives', 'derive', 'derived', 'deriving', 'descendants', 'descent', 'describe', 'described', 'describes', 'describing', 'description', 'descriptions', 'descriptor', 'descriptors', 'design', 'designed', 'designer', 'designing', 'designs', 'desirable', 'desire', 'desired', 'despite', 'det', 'detail', 'detailed', 'details', 'detect', 'detected', 'detecting', 'detection', 'detections', 'detector', 'detectors', 'determinant', 'determination', 'determine', 'determined', 'determines', 'determining', 'deterministic', 'deterministically', 'dev', 'develop', 'developed', 'developing', 'development', 'developments', 'deviate', 'deviation', 'deviations', 'device', 'devise', 'devised', 'devoted', 'diabetes', 'diag', 'diagnosis', 'diagnostic', 'diagonal', 'diagram', 'diagrams', 'diameter', 'diction', 'dictionaries', 'dictionary', 'diego', 'dient', 'dietterich', 'dif', 'diff', 'differ', 'difference', 'differences', 'different', 'differentiable', 'differential', 'differentially', 'differentiate', 'differentiation', 'differently', 'differing', 'differs', 'difficult', 'difficulties', 'difficulty', 'diffusion', 'diffusions', 'digit', 'digital', 'digits', 'dii', 'dim', 'dimen', 'dimension', 'dimensional', 'dimensionality', 'dimensions', 'diminishing', 'ding', 'dir', 'dirac', 'direct', 'directed', 'direction', 'directional', 'directions', 'directly', 'dirichlet', 'dis', 'disadvantage', 'disagreement', 'disc', 'discard', 'discarded', 'discarding', 'discards', 'discharge', 'disconnected', 'discount', 'discounted', 'discounting', 'discover', 'discovered', 'discovering', 'discovers', 'discovery', 'discrepancy', 'discrete', 'discretization', 'discretize', 'discretized', 'discretizing', 'discriminant', 'discriminate', 'discriminating', 'discrimination', 'discriminative', 'discriminatively', 'discuss', 'discussed', 'discusses', 'discussing', 'discussion', 'discussions', 'disease', 'diseases', 'dish', 'dishes', 'disjoint', 'disorder', 'display', 'displayed', 'displays', 'dissimilar', 'dissimilarity', 'dist', 'distance', 'distances', 'distant', 'distinct', 'distinction', 'distinctions', 'distinctive', 'distinguish', 'distinguished', 'distractor', 'distractors', 'distri', 'distribu', 'distributed', 'distribution', 'distributional', 'distributions', 'ditional', 'diverge', 'divergence', 'divergences', 'diverse', 'diversity', 'divide', 'divided', 'divides', 'dividing', 'division', 'divisive', 'document', 'documents', 'doesnt', 'dog', 'doi', 'domain', 'domains', 'dominant', 'dominated', 'dominates', 'domingos', 'done', 'dong', 'donoghue', 'donoho', 'dont', 'dot', 'dots', 'dotted', 'double', 'doubling', 'doubly', 'downloaded', 'dpm', 'dramatic', 'dramatically', 'drastically', 'draw', 'drawback', 'drawbacks', 'drawing', 'drawn', 'draws', 'drew', 'drift', 'drifting', 'drineas', 'drive', 'driven', 'drives', 'driving', 'drop', 'dropped', 'drops', 'drug', 'drugs', 'dual', 'duality', 'duced', 'duchi', 'duke', 'dunson', 'duplicate', 'duration', 'durations', 'dynamic', 'dynamical', 'dynamically', 'dynamics', 'ear', 'earlier', 'early', 'ease', 'easier', 'easily', 'east', 'easy', 'economic', 'economics', 'edge', 'edges', 'edition', 'editor', 'editors', 'education', 'effect', 'effective', 'effectively', 'effectiveness', 'effects', 'effi', 'efficacies', 'efficacy', 'efficiency', 'efficient', 'efficiently', 'effort', 'efforts', 'efron', 'efros', 'eigen', 'eigendecomposition', 'eigenmaps', 'eigenvalue', 'eigenvalues', 'eigenvector', 'eigenvectors', 'either', 'elaborate', 'elad', 'elastic', 'electric', 'electrical', 'electrode', 'electrodes', 'electron', 'electronic', 'electronics', 'elegant', 'element', 'elementary', 'elements', 'eliminate', 'eliminated', 'elimination', 'ellipsoid', 'elliptical', 'els', 'else', 'elsewhere', 'embed', 'embedded', 'embedding', 'embeddings', 'emerged', 'emergence', 'emission', 'emnlp', 'emphasis', 'emphasize', 'empirical', 'empirically', 'employ', 'employed', 'employing', 'employs', 'empty', 'enable', 'enables', 'enabling', 'ence', 'enclosed', 'enclosing', 'encode', 'encoded', 'encoder', 'encoders', 'encodes', 'encoding', 'encompasses', 'encountered', 'encourage', 'encourages', 'encouraging', 'endfor', 'ending', 'ends', 'energies', 'energy', 'enforce', 'enforced', 'enforces', 'enforcing', 'engine', 'engineering', 'engines', 'english', 'enhance', 'enhanced', 'enjoy', 'enjoys', 'enough', 'ens', 'ensemble', 'ensembles', 'ensure', 'ensures', 'ensuring', 'ent', 'entails', 'enter', 'entire', 'entirely', 'entities', 'entity', 'entries', 'entropic', 'entropies', 'entropy', 'entry', 'enumerate', 'enumeration', 'envelope', 'envelopes', 'environment', 'environmental', 'environments', 'episode', 'episodes', 'episodic', 'epoch', 'epochs', 'eqn', 'eqs', 'equa', 'equal', 'equality', 'equally', 'equals', 'equations', 'equilibrium', 'equipped', 'equivalence', 'equivalent', 'equivalently', 'erdo', 'ergodicity', 'eric', 'erm', 'ern', 'error', 'errors', 'escape', 'especially', 'ess', 'essential', 'essentially', 'establish', 'established', 'establishes', 'establishing', 'esti', 'estima', 'estimate', 'estimated', 'estimates', 'estimating', 'estimation', 'estimations', 'estimator', 'estimators', 'etc', 'euclidean', 'europe', 'european', 'eval', 'evaluate', 'evaluated', 'evaluates', 'evaluating', 'evaluation', 'evaluations', 'even', 'evenly', 'event', 'events', 'eventually', 'ever', 'everingham', 'every', 'everywhere', 'evgeniou', 'evidence', 'evident', 'evoked', 'evolution', 'evolutionary', 'evolve', 'evolves', 'evolving', 'exact', 'exactly', 'exam', 'examine', 'examined', 'examining', 'example', 'examples', 'exceed', 'exceeds', 'excellence', 'excellent', 'except', 'exception', 'exceptions', 'excess', 'exchange', 'exchangeability', 'exchangeable', 'excitation', 'excitatory', 'excluded', 'excluding', 'exclusive', 'exclusively', 'execute', 'executed', 'executes', 'executing', 'execution', 'exemplar', 'exemplars', 'exercise', 'exhaustive', 'exhaustively', 'exhibit', 'exhibited', 'exhibits', 'exist', 'existence', 'existing', 'exists', 'exp', 'expand', 'expanded', 'expanding', 'expands', 'expansion', 'expansions', 'expect', 'expectation', 'expectations', 'expected', 'expense', 'expensive', 'exper', 'experi', 'experience', 'experiences', 'experiment', 'experimental', 'experimentally', 'experimentation', 'experimented', 'experiments', 'expert', 'expertise', 'experts', 'explain', 'explained', 'explaining', 'explains', 'explanation', 'explanations', 'explanatory', 'explicit', 'explicitly', 'exploit', 'exploitation', 'exploited', 'exploiting', 'exploits', 'exploration', 'explore', 'explored', 'explores', 'exploring', 'expo', 'exponen', 'exponent', 'exponential', 'exponentially', 'exponentiated', 'exposed', 'exposition', 'exposure', 'express', 'expressed', 'expresses', 'expressing', 'expression', 'expressions', 'expressive', 'expx', 'extend', 'extended', 'extending', 'extends', 'extension', 'extensions', 'extensive', 'extensively', 'extent', 'external', 'extra', 'extracellular', 'extract', 'extracted', 'extracting', 'extraction', 'extremal', 'extreme', 'extremely', 'extrinsic', 'eye', 'face', 'facebook', 'faced', 'faces', 'facial', 'facilitate', 'facing', 'fact', 'factor', 'factored', 'factorial', 'factorisation', 'factorization', 'factorizations', 'factorize', 'factorized', 'factorizes', 'factors', 'facts', 'fail', 'fails', 'failure', 'failures', 'fair', 'fairly', 'fall', 'falling', 'falls', 'false', 'familiar', 'familiarity', 'families', 'family', 'fan', 'far', 'farther', 'farthest', 'fashion', 'fast', 'faster', 'fastest', 'fatigue', 'favor', 'favorable', 'favorably', 'favors', 'fazel', 'feasibility', 'feasible', 'feature', 'features', 'feb', 'february', 'feed', 'feedback', 'feedforward', 'fei', 'fellowship', 'felzenszwalb', 'ference', 'ferent', 'fergus', 'fernando', 'fewer', 'fication', 'field', 'fields', 'fig', 'figs', 'figure', 'figures', 'file', 'fill', 'filter', 'filtered', 'filtering', 'filters', 'final', 'finally', 'finance', 'financial', 'finding', 'findings', 'finds', 'fine', 'finer', 'finger', 'fingers', 'finish', 'finite', 'fire', 'firing', 'firm', 'first', 'firstly', 'fischer', 'fisher', 'fista', 'fit', 'fits', 'fitted', 'five', 'fixed', 'fixing', 'flat', 'flexibility', 'flexible', 'flickr', 'flip', 'floor', 'flop', 'flow', 'flower', 'flows', 'fluctuations', 'fluid', 'fly', 'focs', 'focus', 'focused', 'focuses', 'focusing', 'fol', 'fold', 'folds', 'follow', 'followed', 'following', 'follows', 'food', 'force', 'forced', 'forces', 'forcing', 'forecaster', 'forecasting', 'foreground', 'forest', 'forgetting', 'fork', 'form', 'formal', 'formalism', 'formalize', 'formalized', 'formally', 'formance', 'formation', 'formed', 'former', 'forming', 'forms', 'formula', 'formulas', 'formulate', 'formulated', 'formulating', 'formulation', 'formulations', 'forsyth', 'fortunately', 'forward', 'found', 'foundation', 'foundations', 'four', 'fourier', 'fourth', 'fowlkes', 'fraction', 'fractional', 'frame', 'frames', 'framework', 'frameworks', 'france', 'francis', 'francisco', 'frank', 'free', 'freedom', 'freely', 'freeman', 'french', 'frequencies', 'frequency', 'frequent', 'frequently', 'fresh', 'freund', 'frey', 'friedman', 'friends', 'frobenius', 'front', 'frontiers', 'fukumizu', 'full', 'fully', 'function', 'functional', 'functionals', 'functions', 'fund', 'fundamental', 'fundamentally', 'funded', 'funding', 'fur', 'furthermore', 'fused', 'fusion', 'future', 'gabor', 'gain', 'gained', 'gains', 'gam', 'game', 'games', 'gamma', 'ganglion', 'gap', 'gape', 'gaps', 'gather', 'gathered', 'gating', 'gatsby', 'gaus', 'gauss', 'gaussian', 'gaussians', 'gave', 'geer', 'gence', 'gender', 'gene', 'general', 'generalisation', 'generalised', 'generality', 'generalization', 'generalizations', 'generalize', 'generalized', 'generalizes', 'generalizing', 'generally', 'generate', 'generated', 'generates', 'generating', 'generation', 'generative', 'generator', 'generators', 'generic', 'genes', 'genetic', 'genetics', 'genome', 'genomes', 'genomic', 'genotype', 'genre', 'genres', 'gentile', 'geodesic', 'geoffrey', 'geometric', 'geometrical', 'geometrically', 'geometry', 'george', 'german', 'germany', 'gerstner', 'get', 'gets', 'getting', 'ghahramani', 'ghaoui', 'ghavamzadeh', 'ghz', 'gibbs', 'gin', 'girolami', 'girshick', 'gis', 'gist', 'git', 'give', 'given', 'gives', 'giving', 'glass', 'glasso', 'global', 'globally', 'glucose', 'gmm', 'gmms', 'goal', 'goals', 'goes', 'going', 'gold', 'gong', 'good', 'goodness', 'google', 'gool', 'gordon', 'gorithm', 'gould', 'governed', 'government', 'gps', 'grade', 'gradient', 'gradients', 'gradually', 'graduate', 'grafting', 'grained', 'gram', 'grammar', 'grammars', 'grant', 'grants', 'granularity', 'graph', 'graphical', 'graphics', 'graphs', 'grass', 'grassmann', 'grauman', 'gray', 'great', 'greater', 'greatly', 'greedily', 'greedy', 'green', 'gression', 'gretton', 'grey', 'grid', 'gridworld', 'griffiths', 'gross', 'ground', 'grounding', 'group', 'grouped', 'grouping', 'groupings', 'groups', 'grow', 'growing', 'grows', 'growth', 'guarantee', 'guaranteed', 'guarantees', 'guess', 'guestrin', 'guide', 'guided', 'gupta', 'gxi', 'haifa', 'halfspace', 'halfspaces', 'hall', 'hamilton', 'hamiltonian', 'hamming', 'han', 'hand', 'handbook', 'handle', 'handled', 'handling', 'hands', 'handwriting', 'handwritten', 'hanneke', 'hansen', 'haplotypes', 'happen', 'happens', 'hard', 'harder', 'hardly', 'hardness', 'harmonic', 'harvard', 'hash', 'hashed', 'hashing', 'hastie', 'hastings', 'hat', 'haussler', 'hayes', 'hazan', 'hazard', 'hdp', 'head', 'health', 'heap', 'heart', 'heat', 'heavily', 'heavy', 'hebert', 'hedge', 'heidelberg', 'height', 'hein', 'held', 'help', 'helpful', 'helps', 'hence', 'henceforth', 'heskes', 'hessian', 'heterogeneous', 'heteroscedastic', 'heuristic', 'heuristics', 'hidden', 'hierar', 'hierarchical', 'hierarchically', 'hierarchies', 'hierarchy', 'high', 'higher', 'highest', 'highlight', 'highlighted', 'highlights', 'highly', 'highway', 'hilbert', 'hill', 'hindsight', 'hinge', 'hinton', 'hippocampal', 'hippocampus', 'histogram', 'histograms', 'histories', 'history', 'hit', 'hits', 'hiv', 'hmc', 'hmm', 'hmms', 'hoc', 'hoeffding', 'hoeffdings', 'hoffman', 'hofmann', 'hog', 'hoiem', 'hold', 'holding', 'holds', 'holistic', 'home', 'homeostasis', 'homeostatic', 'homogeneity', 'homogeneous', 'hop', 'hope', 'hopkins', 'hops', 'horizon', 'horizons', 'horizontal', 'horn', 'horse', 'hot', 'hour', 'hours', 'house', 'housing', 'hoyer', 'hsieh', 'huge', 'hull', 'human', 'humans', 'hundred', 'hundreds', 'hybrid', 'hyper', 'hypergraph', 'hyperparameter', 'hyperparameters', 'hyperplane', 'hyperplanes', 'hyperspectral', 'hypotheses', 'hypothesis', 'hypothesized', 'hypothetical', 'ibp', 'ica', 'icml', 'idea', 'ideal', 'ideally', 'ideas', 'identical', 'identically', 'identifiability', 'identifiable', 'identification', 'identified', 'identifies', 'identify', 'identifying', 'identities', 'identity', 'ignore', 'ignored', 'ignores', 'ignoring', 'iid', 'illumination', 'illustrate', 'illustrated', 'illustrates', 'illustrating', 'illustration', 'illustrative', 'image', 'imagenet', 'images', 'imagine', 'imaging', 'immediate', 'immediately', 'impact', 'imperfect', 'implement', 'implementation', 'implementations', 'implemented', 'implementing', 'implements', 'implication', 'implications', 'implicit', 'implicitly', 'implied', 'implies', 'imply', 'implying', 'importance', 'important', 'importantly', 'impose', 'imposed', 'imposes', 'imposing', 'impossible', 'impractical', 'impression', 'improve', 'improved', 'improvement', 'improvements', 'improves', 'improving', 'imputation', 'inaccurate', 'inactive', 'inappropriate', 'inc', 'include', 'included', 'includes', 'including', 'inclusion', 'incoherence', 'incoherent', 'incoming', 'incomplete', 'inconsistent', 'incorporate', 'incorporated', 'incorporates', 'incorporating', 'incorrect', 'incorrectly', 'increase', 'increased', 'increases', 'increasing', 'increasingly', 'increment', 'incremental', 'incrementally', 'increments', 'incur', 'incurred', 'incurs', 'inde', 'indeed', 'indepen', 'independence', 'independent', 'independently', 'index', 'indexed', 'indexes', 'indexing', 'indian', 'indicate', 'indicated', 'indicates', 'indicating', 'indication', 'indicator', 'indicators', 'indices', 'indirect', 'indirectly', 'individual', 'individually', 'individuals', 'indoor', 'induce', 'induced', 'induces', 'inducing', 'induction', 'inductive', 'industrial', 'industry', 'ineffective', 'inefficient', 'inequalities', 'inequality', 'inexact', 'inf', 'infeasible', 'infer', 'inference', 'inferences', 'inferential', 'inferior', 'inferred', 'inferring', 'infimum', 'infinite', 'infinitely', 'infinitesimal', 'infinity', 'influence', 'influenced', 'influences', 'influential', 'inform', 'informa', 'informally', 'informatics', 'information', 'informative', 'informed', 'ing', 'ingredient', 'ingredients', 'inherent', 'inherently', 'inherits', 'inhibition', 'inhibitory', 'inhomogeneous', 'initial', 'initialization', 'initializations', 'initialize', 'initialized', 'initializing', 'initially', 'initiation', 'injected', 'injective', 'inlier', 'inliers', 'inn', 'inner', 'inpainting', 'input', 'inputs', 'inria', 'insensitive', 'inseparable', 'inside', 'insight', 'insights', 'inspection', 'inspired', 'instance', 'instances', 'instantaneous', 'instantiate', 'instantiation', 'instead', 'institute', 'instrumental', 'insufficient', 'int', 'integer', 'integers', 'integral', 'integrals', 'integrate', 'integrated', 'integrates', 'integrating', 'integration', 'intel', 'intell', 'intelligence', 'intelligent', 'intensities', 'intensity', 'intensive', 'inter', 'interact', 'interacting', 'interaction', 'interactions', 'interactive', 'interacts', 'interdependent', 'interest', 'interested', 'interesting', 'interestingly', 'interests', 'interface', 'interfaces', 'interference', 'interior', 'intermediate', 'interna', 'internal', 'international', 'internet', 'interpolation', 'interpret', 'interpretability', 'interpretable', 'interpretation', 'interpretations', 'interpreted', 'interpreting', 'interscience', 'intersect', 'intersection', 'interval', 'intervals', 'intl', 'intra', 'intracellular', 'intractable', 'intrinsic', 'intro', 'introduce', 'introduced', 'introduces', 'introducing', 'introduction', 'intuition', 'intuitive', 'intuitively', 'invariance', 'invariant', 'inverse', 'inversion', 'inverted', 'invertible', 'inverting', 'investigate', 'investigated', 'investigating', 'investigation', 'invoked', 'involve', 'involved', 'involves', 'involving', 'ion', 'ionic', 'ions', 'irl', 'irregular', 'irrelevant', 'irrespective', 'irvine', 'isa', 'ising', 'isolated', 'isolation', 'isometry', 'isotonic', 'isotropic', 'israel', 'issn', 'issue', 'issues', 'ist', 'italy', 'item', 'items', 'iterate', 'iterated', 'iterates', 'iterating', 'iteration', 'iterations', 'iterative', 'iteratively', 'jacob', 'jacobian', 'jain', 'james', 'jan', 'january', 'japan', 'jean', 'jebara', 'jenatton', 'jensen', 'jensens', 'jiang', 'jitter', 'jmlr', 'joachims', 'job', 'john', 'johnson', 'joining', 'joint', 'jointly', 'jones', 'jordan', 'jose', 'joseph', 'journal', 'judged', 'judgment', 'judgments', 'juditsky', 'july', 'jump', 'jumping', 'jumps', 'jun', 'junction', 'june', 'just', 'justification', 'justified', 'justify', 'kakade', 'kalai', 'kale', 'kalman', 'kanade', 'kaufmann', 'kavukcuoglu', 'kdd', 'kde', 'kearns', 'keep', 'keeping', 'keeps', 'kemp', 'kept', 'kernel', 'kernelized', 'kernels', 'kersting', 'key', 'keyboard', 'keypoints', 'kim', 'kind', 'kinds', 'king', 'kitchen', 'klein', 'kleinberg', 'kluwer', 'kmax', 'know', 'knowing', 'knowledge', 'known', 'knows', 'kohli', 'kolmogorov', 'koltchinskii', 'krause', 'kronecker', 'kulis', 'kulkarni', 'kullback', 'kumar', 'lab', 'label', 'labeled', 'labeler', 'labelers', 'labeling', 'labelings', 'labelled', 'labelling', 'labelme', 'labels', 'laboratory', 'labs', 'lack', 'lafferty', 'lag', 'lagoudakis', 'lagrange', 'lagrangian', 'lai', 'lambda', 'lamblin', 'lampert', 'lanckriet', 'landmark', 'landmarks', 'landscape', 'lane', 'langford', 'language', 'languages', 'laplace', 'laplacian', 'laplacians', 'large', 'largely', 'larger', 'largest', 'larochelle', 'lasso', 'last', 'lastly', 'lated', 'latent', 'later', 'lateral', 'latter', 'lattice', 'lattices', 'lausanne', 'law', 'lawrence', 'laws', 'layer', 'layered', 'layers', 'layout', 'lazebnik', 'lazy', 'lbp', 'lcc', 'lda', 'ldpc', 'lds', 'lead', 'leader', 'leading', 'leads', 'leaf', 'leak', 'leapfrog', 'learn', 'learnability', 'learnable', 'learned', 'learner', 'learners', 'learning', 'learns', 'learnt', 'least', 'leave', 'leaves', 'leaving', 'lebesgue', 'lecture', 'lecun', 'led', 'lee', 'left', 'legal', 'legs', 'leibler', 'lem', 'lemma', 'lemmas', 'lems', 'length', 'lengths', 'lengyel', 'leskovec', 'less', 'let', 'lets', 'lett', 'letter', 'letters', 'letting', 'level', 'levels', 'leverage', 'leveraging', 'lewis', 'liang', 'liblinear', 'library', 'libsvm', 'lie', 'lies', 'life', 'lifetime', 'lifted', 'light', 'like', 'likelihood', 'likelihoods', 'likely', 'likewise', 'lille', 'lim', 'limit', 'limitation', 'limitations', 'limited', 'limiting', 'limits', 'line', 'linear', 'linearity', 'linearization', 'linearized', 'linearly', 'lines', 'ling', 'linguistics', 'link', 'linkage', 'linked', 'linking', 'links', 'lipschitz', 'list', 'listed', 'lists', 'literals', 'literature', 'little', 'littlestone', 'littman', 'liu', 'live', 'lives', 'living', 'llc', 'lle', 'lmnn', 'lnk', 'lnp', 'lnt', 'load', 'loading', 'local', 'locality', 'localization', 'localize', 'localized', 'locally', 'located', 'location', 'locations', 'lock', 'locking', 'log', 'logarithm', 'logarithmic', 'logd', 'logic', 'logical', 'logistic', 'logit', 'logk', 'logx', 'london', 'long', 'longer', 'longest', 'longitudinal', 'look', 'looked', 'looking', 'looks', 'lookup', 'loop', 'loops', 'loopy', 'loose', 'loosely', 'los', 'lose', 'loss', 'losses', 'lost', 'lot', 'lova', 'lovasz', 'low', 'lowe', 'lower', 'lowest', 'lowing', 'lsvm', 'lunch', 'luxburg', 'lyapunov', 'lying', 'macaque', 'mach', 'machine', 'machinery', 'machines', 'mackay', 'macke', 'macro', 'made', 'madison', 'mae', 'magnitude', 'magnitudes', 'mahalanobis', 'mahoney', 'mail', 'main', 'mainly', 'maintain', 'maintained', 'maintaining', 'maintains', 'mairal', 'majority', 'make', 'maker', 'makes', 'making', 'mal', 'malicious', 'malik', 'man', 'management', 'mance', 'manfred', 'manifold', 'manifolds', 'manipulations', 'manner', 'manning', 'mannor', 'mansour', 'manual', 'manually', 'manuscript', 'many', 'map', 'mapped', 'mapping', 'mappings', 'mapreduce', 'maps', 'mar', 'march', 'margin', 'marginal', 'marginalization', 'marginalized', 'marginalizing', 'marginals', 'margins', 'mark', 'marked', 'markers', 'market', 'markets', 'markov', 'markovian', 'martin', 'martingale', 'martingales', 'mas', 'mass', 'massachusetts', 'massive', 'master', 'match', 'matched', 'matches', 'matching', 'mate', 'material', 'materials', 'math', 'mathematical', 'mathematically', 'mathematics', 'mation', 'matlab', 'matrices', 'matrix', 'matter', 'matters', 'matthew', 'matthias', 'max', 'maxa', 'maxent', 'maxi', 'maximal', 'maximally', 'maximization', 'maximize', 'maximized', 'maximizer', 'maximizes', 'maximizing', 'maximum', 'may', 'mbp', 'mcallester', 'mccallum', 'mcmc', 'mean', 'meaning', 'meaningful', 'means', 'meant', 'meanwhile', 'measurable', 'measure', 'measured', 'measurement', 'measurements', 'measures', 'measuring', 'mechanical', 'mechanics', 'mechanism', 'mechanisms', 'mechanistic', 'med', 'media', 'median', 'medical', 'medicine', 'medium', 'meeting', 'meier', 'mellon', 'member', 'members', 'membership', 'membrane', 'memories', 'memory', 'men', 'mendelson', 'ment', 'mental', 'mention', 'mentioned', 'mentions', 'ments', 'merely', 'merge', 'merged', 'merging', 'mesh', 'message', 'messages', 'met', 'meta', 'metabolic', 'metadata', 'meth', 'method', 'methodological', 'methodology', 'methods', 'metric', 'metrics', 'metropolis', 'micchelli', 'michael', 'micro', 'microarray', 'microscopy', 'microsoft', 'mid', 'middle', 'might', 'mil', 'mild', 'million', 'millions', 'mind', 'mine', 'ming', 'mini', 'minima', 'minimax', 'minimising', 'minimization', 'minimize', 'minimized', 'minimizer', 'minimizers', 'minimizes', 'minimizing', 'minimum', 'mining', 'minka', 'minm', 'minor', 'minus', 'minutes', 'mirror', 'misclassification', 'misclassified', 'mises', 'mismatch', 'miss', 'missed', 'misses', 'missing', 'misspecification', 'mistake', 'mistakes', 'mit', 'mitigate', 'mix', 'mixed', 'mixing', 'mixture', 'mixtures', 'mization', 'mmd', 'mod', 'modal', 'modalities', 'modality', 'mode', 'model', 'modeled', 'modeling', 'modelled', 'modelling', 'models', 'moderate', 'modern', 'modes', 'modification', 'modifications', 'modified', 'modify', 'modifying', 'modular', 'modularity', 'modulated', 'modulation', 'module', 'modules', 'modulus', 'mohri', 'molecular', 'molecules', 'moment', 'moments', 'momentum', 'money', 'monitor', 'monitoring', 'monkey', 'monkeys', 'monocular', 'monotone', 'monotonic', 'monotonically', 'monotonicity', 'monte', 'month', 'months', 'montreal', 'moore', 'moreover', 'morgan', 'mostly', 'motion', 'motions', 'motivate', 'motivated', 'motivates', 'motivating', 'motivation', 'motor', 'mountain', 'mouse', 'move', 'movement', 'movements', 'moves', 'movie', 'movielens', 'movies', 'moving', 'mrf', 'mrfs', 'mri', 'mse', 'msrc', 'much', 'mul', 'multi', 'multiagent', 'multiarmed', 'multiclass', 'multidimensional', 'multilinear', 'multinomial', 'multiple', 'multiplication', 'multiplications', 'multiplicative', 'multiplied', 'multiplier', 'multipliers', 'multiply', 'multiplying', 'multiscale', 'multitask', 'multivariate', 'multiway', 'mumford', 'munos', 'muri', 'murphy', 'murray', 'music', 'must', 'mutations', 'mutual', 'mutually', 'nac', 'naive', 'naively', 'name', 'named', 'namely', 'names', 'narrow', 'nat', 'nathan', 'national', 'natl', 'natural', 'naturalistic', 'naturally', 'nature', 'navigation', 'ncrp', 'neal', 'near', 'nearby', 'nearest', 'nearly', 'necessarily', 'necessary', 'need', 'needed', 'needs', 'negahban', 'negative', 'negatively', 'negativity', 'negligible', 'neighbor', 'neighborhood', 'neighborhoods', 'neighboring', 'neighbors', 'neighbour', 'neither', 'nemirovski', 'ner', 'nerve', 'ness', 'nested', 'nesterov', 'net', 'netflix', 'netherlands', 'nets', 'network', 'networks', 'neu', 'neural', 'neurally', 'neuro', 'neurobiology', 'neuroimage', 'neuroimaging', 'neuron', 'neuronal', 'neurons', 'neurophysiol', 'neurophysiology', 'neurosci', 'neuroscience', 'never', 'nevertheless', 'new', 'newly', 'newman', 'news', 'newton', 'newtons', 'next', 'nice', 'nicely', 'nicolo', 'nips', 'nition', 'niyogi', 'nlp', 'nocedal', 'node', 'nodes', 'noise', 'noiseless', 'noises', 'noisy', 'nominal', 'nonconvex', 'nondecreasing', 'none', 'nonempty', 'nonetheless', 'nonlinear', 'nonlinearities', 'nonlinearity', 'nonnegative', 'nonparametric', 'nonstationary', 'nonterminal', 'nontrivial', 'nonzero', 'nord', 'norm', 'normal', 'normalisation', 'normalised', 'normality', 'normalization', 'normalize', 'normalized', 'normalizing', 'normally', 'normals', 'normative', 'norms', 'north', 'notable', 'notably', 'notation', 'notational', 'notations', 'note', 'noted', 'notes', 'noteworthy', 'nothing', 'notice', 'noting', 'notion', 'notions', 'nov', 'novel', 'novelty', 'november', 'nowozin', 'nserc', 'nuclear', 'null', 'num', 'number', 'numbers', 'numerical', 'numerically', 'numerous', 'obey', 'obeys', 'object', 'objective', 'objectives', 'objects', 'oblivious', 'obozinski', 'obser', 'observable', 'observation', 'observational', 'observations', 'observe', 'observed', 'observer', 'observes', 'observing', 'obtain', 'obtained', 'obtaining', 'obtains', 'obvious', 'obviously', 'occlusion', 'occlusions', 'occur', 'occurred', 'occurrence', 'occurrences', 'occurring', 'occurs', 'oct', 'october', 'odd', 'odds', 'offer', 'offered', 'offers', 'office', 'offline', 'offs', 'offset', 'often', 'old', 'oliva', 'olivier', 'olog', 'ols', 'olshausen', 'omit', 'omitted', 'ones', 'online', 'onset', 'open', 'opens', 'operate', 'operates', 'operating', 'operation', 'operations', 'operator', 'operators', 'opinion', 'opper', 'opportunities', 'opportunity', 'opposed', 'opposite', 'opt', 'optical', 'optima', 'optimal', 'optimality', 'optimally', 'optimisation', 'optimise', 'optimised', 'optimistic', 'optimiza', 'optimization', 'optimizations', 'optimize', 'optimized', 'optimizer', 'optimizers', 'optimizes', 'optimizing', 'optimum', 'option', 'options', 'optspace', 'oracle', 'orbit', 'orbits', 'order', 'ordered', 'ordering', 'orders', 'ordinal', 'ordinary', 'organisms', 'organization', 'organized', 'orientation', 'orientations', 'oriented', 'orig', 'origin', 'original', 'originally', 'ornstein', 'orthogonal', 'orthogonality', 'orthonormal', 'oscillation', 'osindero', 'others', 'otherwise', 'ous', 'outcome', 'outcomes', 'outdoor', 'outer', 'outlier', 'outliers', 'outline', 'outlined', 'outperform', 'outperformed', 'outperforming', 'outperforms', 'output', 'outputs', 'outside', 'overall', 'overcome', 'overcomplete', 'overfit', 'overfitting', 'overhead', 'overlap', 'overlapped', 'overlapping', 'overlaps', 'overview', 'owing', 'oxford', 'pac', 'pace', 'package', 'packing', 'page', 'pagerank', 'pages', 'paid', 'pair', 'paired', 'pairing', 'pairs', 'pairwise', 'pami', 'panel', 'panels', 'paninski', 'paper', 'papers', 'par', 'paradigm', 'parallel', 'parallelism', 'parallelization', 'parallelized', 'param', 'parame', 'parameter', 'parameterization', 'parameterize', 'parameterized', 'parameters', 'parametric', 'parametrization', 'parametrized', 'pared', 'parent', 'parentheses', 'parents', 'pareto', 'paris', 'parity', 'park', 'parr', 'parrilo', 'parse', 'parsing', 'part', 'partial', 'partially', 'participant', 'participants', 'particle', 'particles', 'particular', 'particularly', 'partition', 'partitioned', 'partitioning', 'partitions', 'partly', 'parts', 'parzen', 'pas', 'pascal', 'pass', 'passed', 'passes', 'passing', 'passive', 'past', 'patch', 'patches', 'path', 'paths', 'pathways', 'patient', 'patients', 'pattern', 'patterns', 'paul', 'pay', 'payoff', 'payoffs', 'pca', 'pcc', 'pci', 'pcs', 'pdf', 'peak', 'pearl', 'pedestrian', 'pedro', 'pegasos', 'pen', 'penalization', 'penalize', 'penalized', 'penalizes', 'penalizing', 'penalties', 'penalty', 'pendent', 'pendulum', 'pennsylvania', 'people', 'peoples', 'per', 'perceived', 'percent', 'percentage', 'perception', 'perceptron', 'perceptual', 'pereira', 'perfect', 'perfectly', 'perfor', 'perform', 'performance', 'performances', 'performed', 'performing', 'performs', 'perhaps', 'period', 'periodic', 'periods', 'permit', 'permits', 'permutation', 'permutations', 'perona', 'perplexity', 'persistence', 'persistent', 'person', 'personal', 'personalized', 'perspective', 'perturbation', 'perturbations', 'perturbed', 'pessimistic', 'pet', 'peter', 'peters', 'phase', 'phases', 'phd', 'phenomena', 'phenomenon', 'phone', 'phoneme', 'photo', 'photographs', 'phrase', 'phrases', 'phylogenetic', 'phys', 'physical', 'physics', 'physiological', 'physiology', 'piano', 'picard', 'pick', 'picked', 'picking', 'picks', 'picture', 'pictures', 'piece', 'pieces', 'piecewise', 'pierre', 'pig', 'pillow', 'pima', 'ping', 'pipeline', 'pit', 'pitman', 'pittsburgh', 'pixel', 'pixels', 'place', 'placed', 'placement', 'places', 'placing', 'plain', 'plan', 'planar', 'planck', 'plane', 'planes', 'planner', 'planning', 'plasticity', 'plate', 'platform', 'platt', 'plausibility', 'plausible', 'play', 'played', 'player', 'players', 'playing', 'plays', 'ple', 'please', 'ples', 'plex', 'plexity', 'pling', 'plos', 'plot', 'plots', 'plotted', 'plsa', 'plug', 'plugging', 'plus', 'pmf', 'poggio', 'point', 'pointed', 'points', 'poisson', 'pol', 'pole', 'policies', 'policy', 'poly', 'polyhedral', 'polyn', 'polynomial', 'polynomially', 'pomdp', 'pomdps', 'ponce', 'ponents', 'pontil', 'pool', 'pooled', 'pooling', 'pools', 'poor', 'poorly', 'popular', 'popularity', 'population', 'populations', 'portable', 'portion', 'portions', 'pos', 'pose', 'posed', 'poses', 'position', 'positions', 'positive', 'positively', 'positives', 'positivity', 'possibilities', 'possibility', 'possible', 'possibly', 'post', 'posterior', 'posteriori', 'posteriors', 'postsynaptic', 'potassium', 'potential', 'potentially', 'potentials', 'power', 'powerful', 'practical', 'practically', 'practice', 'practitioners', 'pre', 'preceding', 'precise', 'precisely', 'precision', 'precisions', 'pred', 'predefined', 'predic', 'predict', 'predicted', 'predicting', 'prediction', 'predictions', 'predictive', 'predictor', 'predictors', 'predicts', 'predominant', 'predominantly', 'prefer', 'preferable', 'preference', 'preferences', 'preferred', 'preliminaries', 'preliminary', 'premise', 'preprint', 'preprocessed', 'preprocessing', 'presence', 'presentation', 'presented', 'presenting', 'presents', 'preserve', 'preserved', 'preserves', 'preserving', 'press', 'presumably', 'presynaptic', 'prevalence', 'prevent', 'prevents', 'previous', 'previously', 'pri', 'price', 'prices', 'primal', 'primarily', 'primary', 'primate', 'prime', 'primitive', 'princeton', 'principal', 'principle', 'principled', 'principles', 'prior', 'priori', 'priority', 'priors', 'prism', 'privacy', 'private', 'prize', 'pro', 'proach', 'proaches', 'prob', 'proba', 'probabil', 'probabilistic', 'probabilities', 'probability', 'probable', 'probably', 'probe', 'probit', 'problem', 'problematic', 'problems', 'proc', 'procedure', 'procedures', 'proceed', 'proceeding', 'proceedings', 'proceeds', 'process', 'processed', 'processes', 'processing', 'processor', 'processors', 'produce', 'produced', 'produces', 'producing', 'product', 'production', 'products', 'profile', 'profiles', 'profit', 'program', 'programme', 'programming', 'programs', 'progress', 'progresses', 'progression', 'progressive', 'progressively', 'prohibitive', 'proj', 'project', 'projected', 'projecting', 'projection', 'projections', 'projective', 'projects', 'prominent', 'promise', 'promising', 'pronounced', 'proof', 'proofs', 'prop', 'propagate', 'propagation', 'proper', 'properly', 'properties', 'property', 'proportion', 'proportional', 'proportionally', 'proportions', 'proposal', 'proposals', 'propose', 'proposed', 'proposes', 'proposing', 'proposition', 'propositional', 'protein', 'proteins', 'protocol', 'prototype', 'provable', 'provably', 'prove', 'proved', 'proven', 'proves', 'provide', 'provided', 'provides', 'providing', 'proving', 'prox', 'proximal', 'proximation', 'proximity', 'proxy', 'prune', 'pruned', 'pruning', 'psd', 'pseudo', 'pseudocode', 'pseudomarginals', 'psychological', 'psychologists', 'psychology', 'public', 'publication', 'publications', 'publicly', 'published', 'publishers', 'pull', 'pulled', 'pulls', 'pure', 'purely', 'purpose', 'purposes', 'pursuit', 'push', 'put', 'putational', 'puts', 'putting', 'pyramid', 'pyramidal', 'quad', 'quadratic', 'quadratically', 'quadrature', 'qualitative', 'qualitatively', 'quality', 'quantified', 'quantifies', 'quantify', 'quantile', 'quantiles', 'quantitative', 'quantitatively', 'quantities', 'quantity', 'quantization', 'quantized', 'quantum', 'quasi', 'queries', 'query', 'querying', 'question', 'questions', 'queue', 'quick', 'quickly', 'quite', 'quotient', 'race', 'rademacher', 'radial', 'radius', 'rae', 'raina', 'raised', 'raises', 'rakhlin', 'ral', 'ramanan', 'rameter', 'rameters', 'ramp', 'ran', 'rand', 'random', 'randomization', 'randomized', 'randomly', 'randomness', 'range', 'ranges', 'ranging', 'rank', 'ranked', 'ranking', 'rankings', 'ranks', 'rankx', 'ranzato', 'rao', 'rapid', 'rapidly', 'rare', 'rarely', 'rasmussen', 'rat', 'rate', 'rated', 'rates', 'rather', 'rating', 'ratings', 'ratio', 'rational', 'ratios', 'ravikumar', 'raw', 'rbf', 'rbm', 'rbms', 'rcc', 'rcv', 'reach', 'reachable', 'reached', 'reaches', 'reaching', 'reaction', 'reactions', 'reactive', 'read', 'reader', 'readily', 'reading', 'ready', 'real', 'realistic', 'reality', 'realizable', 'realization', 'realizations', 'realized', 'reals', 'rearranging', 'reason', 'reasonable', 'reasonably', 'reasoning', 'reasons', 'rec', 'recall', 'recalling', 'receive', 'received', 'receiver', 'receives', 'receiving', 'recent', 'recently', 'receptive', 'recht', 'recog', 'recogni', 'recognition', 'recognize', 'recognized', 'recognizing', 'recommend', 'recommendation', 'recommendations', 'recommended', 'recommending', 'recon', 'reconstruct', 'reconstructed', 'reconstructing', 'reconstruction', 'record', 'recorded', 'recording', 'recordings', 'records', 'recover', 'recovered', 'recovering', 'recovers', 'recovery', 'rectangles', 'rectangular', 'recurrent', 'recursion', 'recursive', 'recursively', 'red', 'reduce', 'reduced', 'reduces', 'reducing', 'reduction', 'reductions', 'redundancy', 'redundant', 'refer', 'reference', 'references', 'referred', 'refers', 'refine', 'refined', 'refinement', 'refines', 'refining', 'reflect', 'reflectance', 'reflected', 'reflecting', 'reflects', 'reformulate', 'reformulated', 'reformulation', 'refractory', 'reg', 'regard', 'regarded', 'regarding', 'regardless', 'regards', 'regime', 'regimes', 'region', 'regions', 'regres', 'regression', 'regressions', 'regressive', 'regressor', 'regressors', 'regret', 'regrets', 'regular', 'regularity', 'regularization', 'regularizations', 'regularize', 'regularized', 'regularizer', 'regularizers', 'regularizing', 'regulation', 'regulatory', 'reinforce', 'reinforcement', 'reject', 'rejected', 'rejecting', 'rejection', 'rel', 'rela', 'relate', 'related', 'relatedness', 'relates', 'relating', 'relation', 'relational', 'relations', 'relationship', 'relationships', 'relative', 'relatively', 'relax', 'relaxation', 'relaxations', 'relaxed', 'relaxing', 'relay', 'release', 'released', 'relevance', 'relevant', 'reliability', 'reliable', 'reliably', 'relies', 'rely', 'relying', 'remain', 'remainder', 'remained', 'remaining', 'remains', 'remark', 'remarkably', 'remarks', 'remember', 'reminiscent', 'removal', 'remove', 'removed', 'removes', 'removing', 'ren', 'renders', 'renewal', 'rep', 'repeat', 'repeated', 'repeatedly', 'repeating', 'repeats', 'repetitions', 'replace', 'replaced', 'replacement', 'replaces', 'replacing', 'replications', 'report', 'reported', 'reporting', 'reports', 'repository', 'repre', 'represen', 'represent', 'representa', 'representable', 'representation', 'representational', 'representations', 'representative', 'representativeness', 'represented', 'representing', 'represents', 'reproduce', 'reproducing', 'request', 'requested', 'requests', 'require', 'required', 'requirement', 'requirements', 'requires', 'requiring', 'res', 'resampling', 'rescaled', 'rescaling', 'research', 'researchers', 'resemblance', 'resembles', 'resentation', 'reset', 'residual', 'resistance', 'resistances', 'resolution', 'resolve', 'resolved', 'resort', 'resources', 'resp', 'respect', 'respective', 'respectively', 'respects', 'respond', 'responding', 'responds', 'response', 'responses', 'rest', 'restarts', 'restaurant', 'resting', 'restrict', 'restricted', 'restricting', 'restriction', 'restrictions', 'restrictive', 'result', 'resultant', 'resulted', 'resulting', 'results', 'retain', 'retained', 'retaining', 'retains', 'retina', 'retinal', 'retrieval', 'retrieve', 'retrieved', 'return', 'returned', 'returning', 'returns', 'reuse', 'reuters', 'rev', 'reveal', 'revealed', 'revealing', 'reveals', 'reverse', 'reversed', 'reversible', 'review', 'reviewers', 'reviews', 'revisited', 'reward', 'rewards', 'reweighted', 'rewrite', 'rewritten', 'rhs', 'ric', 'rich', 'richard', 'richardson', 'richer', 'ridge', 'riding', 'riemannian', 'right', 'rigorous', 'rigorously', 'ring', 'rip', 'rise', 'risk', 'risks', 'rithm', 'rithms', 'river', 'rms', 'rmse', 'rna', 'rnd', 'rnk', 'rnm', 'rnn', 'rnp', 'road', 'robbins', 'robert', 'robot', 'robotic', 'robotics', 'robots', 'robust', 'robustness', 'role', 'roles', 'roll', 'rollout', 'rolls', 'ronald', 'rons', 'room', 'rooms', 'root', 'rooted', 'rostamizadeh', 'rotated', 'rotation', 'rotations', 'roth', 'rother', 'rough', 'roughly', 'round', 'rounding', 'rounds', 'routine', 'routines', 'roux', 'row', 'roweis', 'rows', 'roy', 'royal', 'rubin', 'rule', 'rules', 'run', 'running', 'runs', 'runtime', 'russell', 'rvs', 'saddle', 'safe', 'safely', 'sahani', 'said', 'sake', 'salakhutdinov', 'salient', 'sam', 'sample', 'sampled', 'sampler', 'samplers', 'samples', 'sampling', 'sanghavi', 'santa', 'sapiro', 'sat', 'satisfactory', 'satisfied', 'satisfies', 'satisfy', 'satisfying', 'saturation', 'saul', 'save', 'savings', 'saw', 'saxena', 'say', 'says', 'scalability', 'scalable', 'scalar', 'scalars', 'scale', 'scaled', 'scales', 'scaling', 'scalings', 'scalp', 'scan', 'scatter', 'scenario', 'scenarios', 'scene', 'scenes', 'schapire', 'schatten', 'schedule', 'schedules', 'scheduling', 'schema', 'schematic', 'scheme', 'schemes', 'schmid', 'schmidhuber', 'schmidt', 'schneider', 'scho', 'school', 'schools', 'schuurmans', 'schwartz', 'sci', 'science', 'sciences', 'scientific', 'scientists', 'scope', 'score', 'scores', 'scoring', 'scott', 'screen', 'screening', 'sdp', 'sdps', 'search', 'searches', 'searching', 'seattle', 'sec', 'second', 'secondary', 'secondly', 'seconds', 'sect', 'section', 'sections', 'see', 'seed', 'seeger', 'seek', 'seeking', 'seeks', 'seem', 'seemingly', 'seems', 'seen', 'sees', 'seg', 'segment', 'segmentation', 'segmentations', 'segmented', 'segmenting', 'segments', 'sejnowski', 'selec', 'select', 'selected', 'selecting', 'selection', 'selections', 'selective', 'selectivity', 'selector', 'selects', 'self', 'sem', 'semantic', 'semantically', 'semantics', 'semi', 'semidefinite', 'seminal', 'sen', 'senior', 'sense', 'sensible', 'sensing', 'sensitive', 'sensitivity', 'sensor', 'sensors', 'sensory', 'sent', 'sentation', 'sentence', 'sentences', 'sentiment', 'sep', 'separability', 'separable', 'separate', 'separated', 'separately', 'separates', 'separating', 'separation', 'separator', 'separators', 'september', 'sequel', 'sequence', 'sequences', 'sequential', 'sequentially', 'ser', 'serial', 'series', 'serious', 'serve', 'served', 'servedio', 'serves', 'service', 'session', 'sessions', 'set', 'sets', 'setting', 'settings', 'setup', 'seung', 'seven', 'seventh', 'several', 'severe', 'severely', 'sfm', 'sgd', 'shaded', 'shading', 'shalev', 'shall', 'shallow', 'sham', 'shamir', 'shannon', 'shape', 'shaped', 'shapes', 'share', 'shared', 'shares', 'sharing', 'sharp', 'shawe', 'shed', 'shelf', 'shen', 'shenoy', 'shift', 'shifted', 'shifts', 'ship', 'short', 'shortcomings', 'shortest', 'shorthand', 'shortly', 'shot', 'show', 'showed', 'showing', 'shown', 'shows', 'shrink', 'shrinkage', 'shrinking', 'shwartz', 'siam', 'siamese', 'sian', 'side', 'sides', 'sift', 'siggraph', 'sigir', 'sigkdd', 'sigmoid', 'sigmoidal', 'sign', 'signal', 'signaling', 'signals', 'signature', 'signed', 'significance', 'significant', 'significantly', 'signs', 'silva', 'silver', 'simard', 'simi', 'similar', 'similarities', 'similarity', 'similarly', 'simon', 'simoncelli', 'simple', 'simpler', 'simplest', 'simplex', 'simplicity', 'simplification', 'simplified', 'simplifies', 'simplify', 'simplifying', 'simplistic', 'simply', 'simulate', 'simulated', 'simulating', 'simulation', 'simulations', 'simulator', 'simultaneous', 'simultaneously', 'sin', 'since', 'singer', 'singh', 'single', 'singleton', 'singular', 'sink', 'sion', 'sionality', 'sions', 'sis', 'sit', 'site', 'sitting', 'situation', 'situations', 'sity', 'sivic', 'six', 'sixth', 'size', 'sized', 'sizes', 'skeleton', 'sketch', 'skewed', 'skill', 'skills', 'sky', 'slab', 'slack', 'slda', 'sleep', 'slice', 'slices', 'sliding', 'slight', 'slightly', 'slivkins', 'slope', 'slow', 'slower', 'slowly', 'small', 'smallest', 'smith', 'smola', 'smooth', 'smoothed', 'smoother', 'smoothing', 'smoothly', 'smoothness', 'smyth', 'snp', 'snps', 'snr', 'sobolev', 'soc', 'socher', 'social', 'society', 'soda', 'sodium', 'sofa', 'soft', 'softmax', 'software', 'solely', 'solid', 'solution', 'solutions', 'solvable', 'solve', 'solved', 'solver', 'solvers', 'solves', 'solving', 'something', 'sometimes', 'somewhat', 'song', 'sons', 'soon', 'sophisticated', 'sort', 'sorted', 'sorting', 'sought', 'sound', 'sounds', 'source', 'sources', 'south', 'space', 'spaced', 'spaces', 'spacing', 'spain', 'spam', 'spammer', 'spammers', 'span', 'spanned', 'spanning', 'sparse', 'sparseness', 'sparsification', 'sparsity', 'spatial', 'spatially', 'spatio', 'speaker', 'speakers', 'speaking', 'special', 'specialized', 'species', 'specific', 'specifically', 'specification', 'specificity', 'specified', 'specifies', 'specify', 'specifying', 'spectral', 'spectrogram', 'spectrum', 'speech', 'speed', 'speedup', 'speedups', 'spend', 'spent', 'sphere', 'spherical', 'spike', 'spikes', 'spiking', 'spirit', 'spite', 'spline', 'splines', 'split', 'splits', 'splitting', 'spoken', 'sponding', 'spread', 'springer', 'spurious', 'square', 'squared', 'squares', 'srebro', 'sridharan', 'srivastava', 'srm', 'ssvm', 'sta', 'stability', 'stable', 'stacked', 'stacking', 'stage', 'stages', 'stan', 'standard', 'standing', 'stands', 'stanford', 'star', 'start', 'started', 'starting', 'starts', 'stat', 'state', 'stated', 'statement', 'statements', 'states', 'static', 'stating', 'stationarity', 'stationary', 'statis', 'statist', 'statistic', 'statistical', 'statistically', 'statistics', 'status', 'stay', 'stays', 'stc', 'std', 'stdp', 'steady', 'steepest', 'stein', 'stems', 'step', 'stephen', 'steps', 'stepsize', 'stereo', 'stick', 'still', 'stimulation', 'stimuli', 'stimulus', 'stitching', 'stl', 'stoc', 'stochas', 'stochastic', 'stochastically', 'stochasticity', 'stoltz', 'stone', 'stop', 'stopped', 'stopping', 'stops', 'storage', 'store', 'stored', 'stories', 'storing', 'straight', 'straightforward', 'straints', 'strata', 'strategies', 'strategy', 'stratified', 'stream', 'streaming', 'streams', 'street', 'strehl', 'strength', 'strengths', 'strict', 'strictly', 'string', 'strong', 'stronger', 'strongest', 'strongly', 'struc', 'struct', 'structural', 'structure', 'structured', 'structures', 'stuck', 'student', 'students', 'studied', 'studies', 'study', 'studying', 'stumps', 'style', 'sub', 'subgradient', 'subgradients', 'subgraph', 'subject', 'subjective', 'subjects', 'subjectto', 'sublinear', 'submatrices', 'submatrix', 'submitted', 'submodular', 'submodularity', 'subnet', 'suboptimal', 'suboptimality', 'subordinate', 'subproblem', 'subproblems', 'subroutine', 'subscript', 'subsection', 'subsequent', 'subsequently', 'subset', 'subsets', 'subspace', 'subspaces', 'substantial', 'substantially', 'substitute', 'substituting', 'substitution', 'subthreshold', 'subtle', 'subtracted', 'subtracting', 'subtree', 'subtrees', 'succeed', 'succeeds', 'success', 'successful', 'successfully', 'successive', 'successively', 'sudderth', 'suffer', 'suffers', 'suffice', 'suffices', 'sufficient', 'sufficiently', 'suggest', 'suggested', 'suggesting', 'suggestions', 'suggests', 'sugiyama', 'suitable', 'suitably', 'suited', 'sum', 'summable', 'summaries', 'summarization', 'summarize', 'summarized', 'summarizes', 'summary', 'summation', 'summed', 'summing', 'sums', 'sun', 'sup', 'supe', 'super', 'superimposed', 'superior', 'supermodular', 'superpixel', 'superpixels', 'superscript', 'supervised', 'supervision', 'supplement', 'supplemental', 'supplementary', 'supplied', 'support', 'supported', 'supporting', 'supports', 'suppose', 'supposed', 'suppress', 'suppression', 'supremum', 'sur', 'sure', 'surely', 'surface', 'surfaces', 'surprising', 'surprisingly', 'surrogate', 'surround', 'surrounding', 'survey', 'surveys', 'survival', 'susceptible', 'sustained', 'sutton', 'suzuki', 'svd', 'svm', 'svms', 'swap', 'swaps', 'sweep', 'swiss', 'switch', 'switching', 'switzerland', 'symbol', 'symbols', 'symmetric', 'symmetrization', 'symmetry', 'symp', 'symposium', 'synapse', 'synapses', 'synaptic', 'synchronous', 'syntactic', 'synthesis', 'synthetic', 'sys', 'syst', 'system', 'systematic', 'systematically', 'systems', 'szepesva', 'table', 'tables', 'tackle', 'tackled', 'tackling', 'tail', 'tailed', 'tailored', 'tails', 'take', 'tall', 'tan', 'tang', 'tangent', 'tao', 'target', 'targeted', 'targets', 'task', 'taskar', 'tasks', 'tation', 'tational', 'tations', 'taxonomies', 'taxonomy', 'taylor', 'teach', 'teacher', 'teachers', 'teaching', 'team', 'teams', 'tech', 'technical', 'technically', 'technion', 'technique', 'techniques', 'technology', 'teh', 'tell', 'tells', 'temperature', 'temperatures', 'tempered', 'tempering', 'template', 'templates', 'temporal', 'temporally', 'tend', 'tendency', 'tends', 'tenenbaum', 'tens', 'tensor', 'tensors', 'tent', 'tering', 'terior', 'term', 'termed', 'terminal', 'terminate', 'terminated', 'terminates', 'termination', 'terminology', 'terms', 'ters', 'test', 'tested', 'testing', 'tests', 'tewari', 'texas', 'text', 'texts', 'texture', 'thank', 'thanks', 'theo', 'theorem', 'theorems', 'theoretic', 'theoretical', 'theoretically', 'theories', 'theory', 'therapies', 'therapy', 'thesis', 'thick', 'thin', 'things', 'think', 'thinned', 'third', 'thm', 'thomas', 'thompson', 'though', 'thought', 'threshold', 'thresholded', 'thresholding', 'thresholds', 'throughout', 'throughput', 'thrun', 'thus', 'tial', 'tially', 'tibshirani', 'tic', 'tical', 'ticular', 'tied', 'ties', 'tight', 'tighter', 'tightly', 'tightness', 'tikhonov', 'tiling', 'tilted', 'time', 'times', 'timing', 'timit', 'ting', 'tiny', 'tion', 'tional', 'tions', 'tipping', 'tishby', 'tissue', 'tive', 'tively', 'today', 'together', 'token', 'tokens', 'tokyo', 'told', 'tolerance', 'tolerate', 'tom', 'tomography', 'tone', 'tong', 'took', 'tool', 'toolbox', 'tools', 'top', 'topic', 'topics', 'topographic', 'topological', 'topology', 'tor', 'torr', 'torralba', 'tors', 'total', 'totally', 'toward', 'towards', 'toy', 'trace', 'traces', 'track', 'tracked', 'tracker', 'tracking', 'tracks', 'tractability', 'tractable', 'trade', 'tradeoff', 'tradeoffs', 'trades', 'trading', 'traditional', 'traditionally', 'traffic', 'train', 'trained', 'training', 'trains', 'traintest', 'trajectories', 'trajectory', 'trans', 'transactions', 'transductive', 'transfer', 'transferred', 'transferring', 'transform', 'transformation', 'transformations', 'transformed', 'transforming', 'transforms', 'transition', 'transitions', 'translate', 'translated', 'translates', 'translation', 'translations', 'transmission', 'transmitted', 'transpose', 'treat', 'treated', 'treating', 'treatment', 'treatments', 'treats', 'tree', 'trees', 'treewidth', 'trend', 'trends', 'trevor', 'trial', 'trials', 'triangle', 'triangular', 'tribution', 'tributions', 'trick', 'tried', 'tries', 'triggered', 'triggs', 'triplet', 'triplets', 'trivial', 'trivially', 'truck', 'true', 'truly', 'truncated', 'truncation', 'trust', 'truth', 'try', 'trying', 'tsitsiklis', 'tsochantaridis', 'tsybakov', 'tucker', 'tune', 'tuned', 'tuning', 'tuple', 'tuples', 'ture', 'tures', 'turk', 'turn', 'turns', 'tutorial', 'tween', 'twenty', 'twice', 'type', 'types', 'typical', 'typically', 'uhlenbeck', 'ultimately', 'unable', 'unambiguous', 'unary', 'unavoidable', 'unbalanced', 'unbiased', 'unbounded', 'uncertain', 'uncertainty', 'unchanged', 'unclear', 'unconstrained', 'uncorrelated', 'underestimate', 'underlies', 'underlying', 'understand', 'understood', 'undesirable', 'undirected', 'uneven', 'unfolding', 'unfortunately', 'uni', 'unif', 'unified', 'uniform', 'uniformly', 'unify', 'unifying', 'unimodal', 'union', 'unique', 'uniquely', 'uniqueness', 'unit', 'united', 'units', 'univ', 'univariate', 'universal', 'universita', 'universite', 'university', 'unknown', 'unlabeled', 'unless', 'unlike', 'unlikely', 'unnecessary', 'unnormalized', 'unobserved', 'unordered', 'unregularized', 'unrelated', 'unreliable', 'unseen', 'unstable', 'unstructured', 'unsupervised', 'unusual', 'unweighted', 'update', 'updated', 'updates', 'updating', 'upon', 'upper', 'urban', 'url', 'urtasun', 'usa', 'usage', 'use', 'used', 'useful', 'usefulness', 'user', 'users', 'uses', 'using', 'usps', 'usual', 'usually', 'utilities', 'utility', 'utilize', 'utilized', 'utilizes', 'utilizing', 'utterances', 'val', 'valiant', 'valid', 'validate', 'validated', 'validation', 'validity', 'valuable', 'valuation', 'valuations', 'value', 'valued', 'values', 'vancouver', 'vandenberghe', 'vanilla', 'vanish', 'vanishes', 'vanishing', 'vapnik', 'var', 'variability', 'variable', 'variables', 'variance', 'variances', 'variant', 'variants', 'variate', 'variates', 'variation', 'variational', 'variations', 'varied', 'varies', 'variety', 'various', 'vary', 'varying', 'vast', 'vec', 'vector', 'vectors', 'vedaldi', 'vehicle', 'vehicles', 'velocity', 'vempala', 'verified', 'verify', 'verlag', 'versa', 'version', 'versions', 'vert', 'vertex', 'vertical', 'vertices', 'vice', 'vidal', 'video', 'videos', 'view', 'viewed', 'viewing', 'viewpoint', 'views', 'vincent', 'violate', 'violated', 'violates', 'violations', 'virtually', 'vis', 'vishwanathan', 'visibility', 'visible', 'vision', 'visit', 'visited', 'visual', 'visualization', 'visualize', 'visualized', 'visually', 'viterbi', 'vivo', 'vocabulary', 'volatility', 'voltage', 'volume', 'vote', 'voting', 'voxel', 'voxels', 'wainwright', 'wait', 'waiting', 'wake', 'walk', 'walking', 'walks', 'wall', 'walsh', 'want', 'ward', 'warmuth', 'warping', 'washington', 'wasserman', 'water', 'watson', 'wave', 'waveform', 'waveforms', 'wavelet', 'way', 'ways', 'weak', 'weaker', 'weakly', 'web', 'webpages', 'website', 'webspam', 'weight', 'weighted', 'weighting', 'weights', 'weinberger', 'weiss', 'well', 'welling', 'wer', 'west', 'weston', 'whatever', 'whenever', 'whereas', 'whereby', 'wherein', 'whether', 'whilst', 'white', 'whitened', 'whitening', 'wide', 'widely', 'wider', 'width', 'widths', 'wiener', 'wikipedia', 'wild', 'wiley', 'william', 'williams', 'williamson', 'willsky', 'wilson', 'win', 'window', 'windows', 'wine', 'winn', 'wireless', 'wisconsin', 'wise', 'wish', 'wishart', 'wishes', 'within', 'without', 'wolf', 'woods', 'word', 'wordnet', 'words', 'work', 'worked', 'worker', 'workers', 'working', 'works', 'workshop', 'world', 'worse', 'worst', 'worth', 'wright', 'write', 'writing', 'written', 'wrong', 'xing', 'year', 'years', 'yeast', 'yellow', 'york', 'young', 'zhang', 'zhao', 'zhou', 'zhu', 'zilberstein', 'zou', 'zoubin', 'zurich']\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 2 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 3 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6J10Ob-pwc_",
        "colab_type": "code",
        "outputId": "ec973b6a-a44f-4471-80f0-87476641c054",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "total_count = np.sum(bag_of_words)\n",
        "bag_of_words = tf.cast(bag_of_words, dtype=tf.float32)\n",
        "data_size, feature_size = bag_of_words.shape\n",
        "\n",
        "  # Compute expected log-likelihood. First, sample from the variational\n",
        "  # distribution; second, compute the log-likelihood given the sample.\n",
        "qw2, qw1, qw0, qz2, qz1, qz0 = deep_exponential_family_variational(\n",
        "      data_size,\n",
        "      feature_size,\n",
        "      [100, 30, 15])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RandomVariable(\"\n",
            "[[0.76638013 0.72634184 0.7091819  0.54773325 0.85444665 0.6916571\n",
            "  0.5554572  0.8024089  0.61525583 0.6938492  0.7276032  0.59040785\n",
            "  0.8366246  0.8037608  0.80762523 0.8003547  0.55389655 0.591243\n",
            "  0.88974637 0.5595267  0.87698805 0.77876514 0.5947429  0.7203584\n",
            "  0.8795036  0.5671319  0.67417073 0.75732875 0.55896056 0.5371249 ]\n",
            " [0.5504246  0.56875604 0.5691553  0.83667207 0.55526286 0.6242367\n",
            "  0.75939953 0.8617787  0.7627822  0.5386111  0.62205166 0.5339345\n",
            "  0.6016416  0.61345327 0.71098673 0.8518887  0.7600916  0.7139102\n",
            "  0.79141295 0.8593241  0.76448977 0.8064695  0.64174527 0.65692806\n",
            "  0.61968917 0.7453042  0.60552514 0.7882108  0.59880996 0.7486093 ]\n",
            " [0.81350017 0.7006977  0.7461702  0.7829429  0.728034   0.6492633\n",
            "  0.8789792  0.7500843  0.8281819  0.5775149  0.7206584  0.6177919\n",
            "  0.8413499  0.7966154  0.5536822  0.551145   0.6457198  0.7628359\n",
            "  0.8899099  0.6256598  0.7431389  0.55260706 0.65095013 0.73998994\n",
            "  0.56029296 0.8116073  0.535645   0.69593495 0.8290974  0.68764555]\n",
            " [0.53119975 0.7889234  0.63186294 0.85687333 0.6527871  0.63117933\n",
            "  0.72804934 0.7922643  0.8039797  0.88441306 0.57880795 0.6195547\n",
            "  0.76187944 0.64657813 0.6256363  0.5953797  0.8219561  0.54249424\n",
            "  0.7214652  0.6221975  0.7185974  0.55312246 0.70290357 0.6630595\n",
            "  0.7342645  0.81373304 0.82341933 0.7403027  0.86499625 0.572407  ]\n",
            " [0.6566129  0.54338396 0.7238727  0.6055555  0.601039   0.67156094\n",
            "  0.84060943 0.62935096 0.63247055 0.69289535 0.6505712  0.61461014\n",
            "  0.57455456 0.6638603  0.5332831  0.5351056  0.66876256 0.8576546\n",
            "  0.80947894 0.8347694  0.80940825 0.8815738  0.8719148  0.53818846\n",
            "  0.86682403 0.79315865 0.57415557 0.6371771  0.6747054  0.64378226]\n",
            " [0.6985267  0.531232   0.7035692  0.6174099  0.71455467 0.5650343\n",
            "  0.8778866  0.8824873  0.7929707  0.77647954 0.73578334 0.7263727\n",
            "  0.64160883 0.54362464 0.62446123 0.60499084 0.7511308  0.62103355\n",
            "  0.7630555  0.85935974 0.6912513  0.8499633  0.70396274 0.59565824\n",
            "  0.8253194  0.8607985  0.745672   0.75479496 0.6970575  0.7216272 ]\n",
            " [0.657892   0.5361812  0.62109685 0.653337   0.7310135  0.53557456\n",
            "  0.5820627  0.7070521  0.58773166 0.63035667 0.844808   0.6989411\n",
            "  0.5929927  0.6314592  0.8269577  0.69530827 0.5931768  0.60973895\n",
            "  0.67395157 0.54512995 0.67256236 0.7372246  0.6162861  0.698897\n",
            "  0.726646   0.77352846 0.7144054  0.7909999  0.7906449  0.62359965]\n",
            " [0.574238   0.5829717  0.6900023  0.5338852  0.778224   0.877689\n",
            "  0.7447554  0.8369665  0.6194977  0.8468362  0.5594227  0.5652225\n",
            "  0.77165323 0.7441603  0.81600803 0.6940343  0.7572781  0.65898186\n",
            "  0.6413786  0.7760338  0.7743185  0.6223369  0.8410217  0.8692234\n",
            "  0.75337094 0.52726483 0.61213917 0.8814566  0.6254802  0.54115635]\n",
            " [0.8000713  0.6607968  0.7219728  0.88936436 0.7103916  0.665031\n",
            "  0.792369   0.5633799  0.85528255 0.6416822  0.80526847 0.586688\n",
            "  0.79619116 0.7257616  0.7205131  0.79111177 0.5741046  0.82207984\n",
            "  0.7375191  0.53449935 0.57685816 0.5423466  0.59956    0.58890194\n",
            "  0.6611617  0.7222121  0.867867   0.7435709  0.5778006  0.8342923 ]\n",
            " [0.81493914 0.86276686 0.8690748  0.64460903 0.8259255  0.54847884\n",
            "  0.6100896  0.750435   0.82253057 0.7782071  0.5592131  0.8202092\n",
            "  0.6247624  0.8912307  0.6428019  0.8628045  0.8462704  0.6495575\n",
            "  0.54971164 0.58107394 0.8637475  0.7223112  0.6377055  0.56422055\n",
            "  0.6373597  0.6341692  0.55867434 0.82369566 0.8698915  0.6301029 ]\n",
            " [0.6582444  0.5440346  0.5395332  0.69131815 0.6722028  0.65866244\n",
            "  0.6727095  0.6569823  0.5832545  0.5654589  0.65262336 0.7818144\n",
            "  0.7832062  0.7407161  0.6298289  0.7964016  0.77117276 0.79881144\n",
            "  0.60316604 0.73063886 0.7936129  0.6837423  0.74337214 0.6478751\n",
            "  0.5968616  0.87429446 0.64129186 0.5869863  0.558005   0.66537595]\n",
            " [0.5490923  0.6174589  0.5336684  0.8000001  0.732705   0.66890955\n",
            "  0.5726897  0.8316371  0.80020434 0.8047214  0.71541893 0.58936685\n",
            "  0.6028007  0.57451737 0.56921583 0.8828877  0.66369873 0.8151961\n",
            "  0.68535113 0.86577713 0.83920723 0.5474874  0.56169087 0.76348525\n",
            "  0.6201698  0.8809264  0.72729367 0.72823656 0.78546673 0.6190994 ]\n",
            " [0.6681702  0.6049678  0.72672874 0.86491996 0.6439394  0.839008\n",
            "  0.8427961  0.54349077 0.53435844 0.62375486 0.6074785  0.5905125\n",
            "  0.76305795 0.87595725 0.79095787 0.6795795  0.71487105 0.67443895\n",
            "  0.5635496  0.5616685  0.8666233  0.81871605 0.5832719  0.889164\n",
            "  0.55907667 0.66175866 0.6308445  0.66266197 0.6745827  0.6853146 ]\n",
            " [0.87230295 0.7797451  0.6915236  0.5398888  0.6864216  0.74200404\n",
            "  0.5572641  0.67983496 0.73921245 0.8101583  0.5458345  0.5952113\n",
            "  0.56814647 0.63151306 0.74376553 0.5880431  0.6812375  0.73288\n",
            "  0.6451752  0.71093243 0.80070364 0.5796981  0.7040201  0.6143042\n",
            "  0.5757079  0.80978113 0.565289   0.59645426 0.7584498  0.54407406]\n",
            " [0.8812758  0.66444415 0.72161686 0.8734465  0.8762749  0.60730106\n",
            "  0.6364902  0.80196184 0.8108844  0.71164244 0.83233297 0.71605134\n",
            "  0.7884852  0.8149589  0.72577727 0.5490906  0.5437643  0.69892794\n",
            "  0.57771724 0.81670487 0.5791605  0.6050437  0.7691082  0.6532177\n",
            "  0.86717176 0.6201627  0.68712425 0.53796923 0.5613233  0.70777327]]\", shape=(15, 30), dtype=float32, device=/job:localhost/replica:0/task:0/device:CPU:0)\n",
            "RandomVariable(\"\n",
            "[[0.7288696  0.7700474  0.7232476  ... 0.73415816 0.65286136 0.7550924 ]\n",
            " [0.7138246  0.6814417  0.6280777  ... 0.62474155 0.59212667 0.7108221 ]\n",
            " [0.7281981  0.6860485  0.7055104  ... 0.6292336  0.7348102  0.6479192 ]\n",
            " ...\n",
            " [0.65839416 0.61557806 0.7499185  ... 0.67006326 0.7597551  0.68568337]\n",
            " [0.75446373 0.6632619  0.7696891  ... 0.7603092  0.77836037 0.63159245]\n",
            " [0.78255767 0.79685414 0.68105775 ... 0.72484076 0.6210323  0.7134374 ]]\", shape=(30, 100), dtype=float32, device=/job:localhost/replica:0/task:0/device:CPU:0)\n",
            "RandomVariable(\"\n",
            "[[0.6983258  0.69326544 0.68849534 ... 0.6930064  0.68862975 0.69688517]\n",
            " [0.69349176 0.6865286  0.6895568  ... 0.6971895  0.6863414  0.6975093 ]\n",
            " [0.6998777  0.6922476  0.69759476 ... 0.69221336 0.6925116  0.6888525 ]\n",
            " ...\n",
            " [0.6943369  0.6957045  0.6956419  ... 0.68705785 0.693606   0.69972706]\n",
            " [0.6929082  0.6907242  0.69635636 ... 0.69600505 0.6890273  0.69595444]\n",
            " [0.69282454 0.6872528  0.69435954 ... 0.69883895 0.68769115 0.6910288 ]]\", shape=(100, 32000), dtype=float32, device=/job:localhost/replica:0/task:0/device:CPU:0)\n",
            "RandomVariable(\"\n",
            "[[1.0457965e-06 1.9973813e-06 3.4491666e-07 ... 6.2579406e-06\n",
            "  2.1681963e-05 4.3800216e-07]\n",
            " [5.5997880e-06 1.9951192e-06 6.9307689e-06 ... 1.3372703e-06\n",
            "  1.2597503e-07 1.4099719e-07]\n",
            " [1.4415680e-05 4.8431930e-06 9.3350627e-07 ... 1.1334033e-06\n",
            "  3.3860964e-05 1.2462617e-05]\n",
            " ...\n",
            " [4.0300615e-06 1.2445802e-06 2.7073100e-05 ... 2.9340796e-05\n",
            "  7.4090149e-06 7.2159014e-06]\n",
            " [8.0953005e-06 6.4594147e-05 1.3187039e-05 ... 6.6519392e-06\n",
            "  2.9758747e-05 3.1635591e-07]\n",
            " [5.2826558e-06 1.1933075e-06 1.8012670e-05 ... 4.0066141e-05\n",
            "  1.8393295e-05 1.4430146e-06]]\", shape=(256, 15), dtype=float32, device=/job:localhost/replica:0/task:0/device:CPU:0)\n",
            "RandomVariable(\"\n",
            "[[7.2025587e-06 3.0799813e-05 1.0897352e-05 ... 1.1221198e-05\n",
            "  3.9120863e-05 1.5310439e-05]\n",
            " [7.0764770e-07 4.6196701e-07 4.3715459e-06 ... 1.3728019e-05\n",
            "  1.5049212e-05 7.9037491e-06]\n",
            " [1.1522296e-05 1.9484993e-05 1.4605755e-06 ... 6.1996011e-06\n",
            "  5.5831956e-06 2.1417611e-05]\n",
            " ...\n",
            " [9.2083719e-06 1.1280514e-05 6.2800377e-06 ... 3.3360633e-05\n",
            "  2.5561994e-06 1.6246198e-05]\n",
            " [9.9545169e-08 8.0531136e-06 6.3225375e-06 ... 1.1717650e-06\n",
            "  4.8653891e-07 1.2499813e-05]\n",
            " [1.1713758e-05 4.4357671e-06 3.5128902e-05 ... 1.8969205e-05\n",
            "  8.4520925e-06 9.8805231e-07]]\", shape=(256, 30), dtype=float32, device=/job:localhost/replica:0/task:0/device:CPU:0)\n",
            "RandomVariable(\"\n",
            "[[4.1630583e-06 6.6985053e-06 2.8078907e-06 ... 2.6256757e-06\n",
            "  8.4887215e-06 1.9711571e-05]\n",
            " [7.6796550e-06 5.3287226e-06 5.7038610e-06 ... 7.3779738e-08\n",
            "  3.4157714e-05 1.1170553e-05]\n",
            " [6.8891995e-06 1.1716545e-05 6.0526459e-06 ... 1.4858748e-05\n",
            "  8.1348207e-06 2.1649224e-05]\n",
            " ...\n",
            " [4.5607449e-06 3.3521057e-05 4.5616893e-05 ... 5.0828153e-06\n",
            "  2.5310695e-05 3.2469543e-05]\n",
            " [1.6190223e-05 3.8185431e-06 8.6075352e-06 ... 8.7181297e-07\n",
            "  1.0226427e-06 4.1688809e-06]\n",
            " [1.4396521e-06 3.8533799e-05 1.9237965e-05 ... 1.1487756e-05\n",
            "  1.3982960e-05 4.7980875e-06]]\", shape=(256, 100), dtype=float32, device=/job:localhost/replica:0/task:0/device:CPU:0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEpQGc1q0paD",
        "colab_type": "code",
        "outputId": "6ee0da4c-4241-4991-e621-ff6ce17217bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "with ed.tape() as model_tape:\n",
        "    with ed.interception(ed.make_value_setter(w2=qw2, w1=qw1, w0=qw0,\n",
        "                                              z2=qz2, z1=qz1, z0=qz0)):\n",
        "      posterior_predictive = deep_exponential_family(data_size,\n",
        "                                                     feature_size,\n",
        "                                                     [100, 30, 15],\n",
        "                                                     0.1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RandomVariable(\"\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\", shape=(256, 32000), dtype=float32, device=/job:localhost/replica:0/task:0/device:CPU:0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5h2R8gTx0-wC",
        "colab_type": "code",
        "outputId": "f00f696e-f3c9-4dfc-9b56-7e0d6babd1c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "log_likelihood = posterior_predictive.distribution.log_prob(bag_of_words)\n",
        "log_likelihood = tf.reduce_sum(input_tensor=log_likelihood)\n",
        "tf.compat.v1.summary.scalar(\"log_likelihood\", log_likelihood)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: id=5596, shape=(), dtype=string, numpy=b'\\n\\x15\\n\\x0elog_likelihood\\x15\\x1aI\\xa3\\xcd'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywvuGca91Qeu",
        "colab_type": "code",
        "outputId": "b8931631-a3a1-4816-811a-cb185af8ba1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        " # Compute analytic KL-divergence between variational and prior distributions.\n",
        "  kl = 0.\n",
        "  for rv_name, variational_rv in [(\"z0\", qz0), (\"z1\", qz1), (\"z2\", qz2),\n",
        "                                  (\"w0\", qw0), (\"w1\", qw1), (\"w2\", qw2)]:\n",
        "    kl += tf.reduce_sum(\n",
        "        input_tensor=variational_rv.distribution.kl_divergence(\n",
        "            model_tape[rv_name].distribution))\n",
        "\n",
        "  tf.compat.v1.summary.scalar(\"kl\", kl)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: id=5704, shape=(), dtype=string, numpy=b'\\n\\t\\n\\x02kl\\x15\\xc5\\x95\\xddJ'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gE5uB8dW1WhI",
        "colab_type": "code",
        "outputId": "ea53e7a3-e6c8-4340-dc3a-a71283ae7e57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "source": [
        "elbo = log_likelihood - kl\n",
        "tf.compat.v1.summary.scalar(\"elbo\", elbo)\n",
        "optimizer = tf.compat.v1.train.AdamOptimizer(1e-4)\n",
        "\n",
        "train_op = optimizer.minimize(-elbo)\n",
        "# sess = tf.compat.v1.Session()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-85-24bda3a2c036>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0melbo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m# sess = tf.compat.v1.Session()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/optimizer.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)\u001b[0m\n\u001b[1;32m    401\u001b[0m         \u001b[0maggregation_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation_method\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0mcolocate_gradients_with_ops\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolocate_gradients_with_ops\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m         grad_loss=grad_loss)\n\u001b[0m\u001b[1;32m    404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m     \u001b[0mvars_with_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrads_and_vars\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/optimizer.py\u001b[0m in \u001b[0;36mcompute_gradients\u001b[0;34m(self, loss, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, grad_loss)\u001b[0m\n\u001b[1;32m    479\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m       raise RuntimeError(\n\u001b[0;32m--> 481\u001b[0;31m           \u001b[0;34m\"`loss` passed to Optimizer.compute_gradients should \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    482\u001b[0m           \"be a function when eager execution is enabled.\")\n\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: `loss` passed to Optimizer.compute_gradients should be a function when eager execution is enabled."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ehhKImMm8o0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "# def main(argv):\n",
        "#   del argv  # unused\n",
        "#   FLAGS.layer_sizes = [int(layer_size) for layer_size in FLAGS.layer_sizes]\n",
        "#   if len(FLAGS.layer_sizes) != 3:\n",
        "#     raise NotImplementedError(\"Specifying fewer or more than 3 layers is not \"\n",
        "#                               \"currently available.\")\n",
        "#   if tf.io.gfile.exists(FLAGS.model_dir):\n",
        "#     tf.compat.v1.logging.warning(\n",
        "#         \"Warning: deleting old log directory at {}\".format(FLAGS.model_dir))\n",
        "#     tf.io.gfile.rmtree(FLAGS.model_dir)\n",
        "#   tf.io.gfile.makedirs(FLAGS.model_dir)\n",
        "\n",
        "#   if FLAGS.fake_data:\n",
        "#     bag_of_words = np.random.poisson(1., size=[10, 25])\n",
        "#     words = [str(i) for i in range(25)]\n",
        "#   else:\n",
        "#     bag_of_words, words = load_nips2011_papers(FLAGS.data_dir)\n",
        "\n",
        "#   total_count = np.sum(bag_of_words)\n",
        "#   bag_of_words = tf.cast(bag_of_words, dtype=tf.float32)\n",
        "#   data_size, feature_size = bag_of_words.shape\n",
        "\n",
        "#   # Compute expected log-likelihood. First, sample from the variational\n",
        "#   # distribution; second, compute the log-likelihood given the sample.\n",
        "#   qw2, qw1, qw0, qz2, qz1, qz0 = deep_exponential_family_variational(\n",
        "#       data_size,\n",
        "#       feature_size,\n",
        "#       FLAGS.layer_sizes)\n",
        "\n",
        "#   with ed.tape() as model_tape:\n",
        "#     with ed.interception(ed.make_value_setter(w2=qw2, w1=qw1, w0=qw0,\n",
        "#                                               z2=qz2, z1=qz1, z0=qz0)):\n",
        "#       posterior_predictive = deep_exponential_family(data_size,\n",
        "#                                                      feature_size,\n",
        "#                                                      FLAGS.layer_sizes,\n",
        "#                                                      FLAGS.shape)\n",
        "\n",
        "#   log_likelihood = posterior_predictive.distribution.log_prob(bag_of_words)\n",
        "#   log_likelihood = tf.reduce_sum(input_tensor=log_likelihood)\n",
        "#   tf.compat.v1.summary.scalar(\"log_likelihood\", log_likelihood)\n",
        "\n",
        "#   # Compute analytic KL-divergence between variational and prior distributions.\n",
        "#   kl = 0.\n",
        "#   for rv_name, variational_rv in [(\"z0\", qz0), (\"z1\", qz1), (\"z2\", qz2),\n",
        "#                                   (\"w0\", qw0), (\"w1\", qw1), (\"w2\", qw2)]:\n",
        "#     kl += tf.reduce_sum(\n",
        "#         input_tensor=variational_rv.distribution.kl_divergence(\n",
        "#             model_tape[rv_name].distribution))\n",
        "\n",
        "#   tf.compat.v1.summary.scalar(\"kl\", kl)\n",
        "\n",
        "#   elbo = log_likelihood - kl\n",
        "#   tf.compat.v1.summary.scalar(\"elbo\", elbo)\n",
        "#   optimizer = tf.compat.v1.train.AdamOptimizer(FLAGS.learning_rate)\n",
        "#   train_op = optimizer.minimize(-elbo)\n",
        "\n",
        "#   sess = tf.compat.v1.Session()\n",
        "#   summary = tf.compat.v1.summary.merge_all()\n",
        "#   summary_writer = tf.compat.v1.summary.FileWriter(FLAGS.model_dir, sess.graph)\n",
        "#   start_time = time.time()\n",
        "\n",
        "#   sess.run(tf.compat.v1.global_variables_initializer())\n",
        "#   for step in range(FLAGS.max_steps):\n",
        "#     start_time = time.time()\n",
        "#     _, elbo_value = sess.run([train_op, elbo])\n",
        "#     if step % 500 == 0:\n",
        "#       duration = time.time() - start_time\n",
        "#       print(\"Step: {:>3d} Loss: {:.3f} ({:.3f} sec)\".format(\n",
        "#           step, elbo_value, duration))\n",
        "#       summary_str = sess.run(summary)\n",
        "#       summary_writer.add_summary(summary_str, step)\n",
        "#       summary_writer.flush()\n",
        "\n",
        "#       # Compute perplexity of the full data set. The model's negative\n",
        "#       # log-likelihood of data is upper bounded by the variational objective.\n",
        "#       negative_log_likelihood = -elbo_value\n",
        "#       perplexity = np.exp(negative_log_likelihood / total_count)\n",
        "#       print(\"Negative log-likelihood <= {:0.3f}\".format(\n",
        "#           negative_log_likelihood))\n",
        "#       print(\"Perplexity <= {:0.3f}\".format(perplexity))\n",
        "\n",
        "#       # Print top 10 words for first 10 topics.\n",
        "#       qw0_values = sess.run(qw0)\n",
        "#       for k in range(min(10, FLAGS.layer_sizes[-1])):\n",
        "#         top_words_idx = qw0_values[k, :].argsort()[-10:][::-1]\n",
        "#         top_words = \" \".join([words[i] for i in top_words_idx])\n",
        "#         print(\"Topic {}: {}\".format(k, top_words))\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#   tf.compat.v1.app.run()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8nFt1gPAEX1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def build_trainable_positive_pointmass(shape, name=None):\n",
        "#   \"\"\"Builds point mass r.v. over positive reals and its parameters.\"\"\"\n",
        "#   mean = tf.Variable(tf.random.normal(shape))\n",
        "#   print(mean)\n",
        "#   def positive_pointmass():\n",
        "#     return tf.nn.softplus(mean)\n",
        "  \n",
        "#   return positive_pointmass, [mean]\n",
        "\n",
        "# def build_trainable_gamma(shape, name=None):\n",
        "#   \"\"\"Builds Gamma random variable and its parameters.\"\"\"\n",
        "#   shape_param = tf.Variable(tf.random.normal(shape))\n",
        "#   scale_param = tf.Variable(tf.random.normal(shape))\n",
        "#   def gamma():\n",
        "#     return ed.Gamma(tf.nn.softplus(shape_param), 1./tf.nn.softplus(scale_param), name=name)\n",
        "#   return gamma, [shape_param, scale_param]\n",
        "\n",
        "# def build_deep_exponential_family_variational():\n",
        "#   \"\"\"Builds posterior approximation q(w{0,1,2}, z{1,2,3} | x) and parameters.\"\"\"\n",
        "#   QW2, qw2_params = build_trainable_positive_pointmass(w2.shape, name=\"qw2\")\n",
        "#   QW1, qw1_params = build_trainable_positive_pointmass(w1.shape, name=\"qw1\")\n",
        "#   QW0, qw0_params = build_trainable_positive_pointmass(w0.shape, name=\"qw0\")\n",
        "#   QZ2, qz2_params = build_trainable_gamma(z2.shape, name=\"qz2\")\n",
        "#   QZ1, qz1_params = build_trainable_gamma(z1.shape, name=\"qz1\")\n",
        "#   QZ0, qz0_params = build_trainable_gamma(z0.shape, name=\"qz0\")\n",
        "#   parameters = (qw2_params + qw1_params + qw0_params +\n",
        "#                 qz2_params + qz1_params + qz0_params)\n",
        "#   def deep_exponential_family_variational():\n",
        "#     return QW2(), QW1(), QW0(), QZ2(), QZ1(), QZ0()\n",
        "#   return deep_exponential_family_variational, parameters\n",
        "\n",
        "# x = build_trainable_positive_pointmass((3,6))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XC8tQ09bIXc3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# max_steps = 10000  # number of training iterations\n",
        "# model_dir = None  # directory for model checkpoints\n",
        "\n",
        "# writer = tf.compat.v1.summary.create_file_writer(\"tmp\")\n",
        "# [deep_exponential_family_variational,\n",
        "#   trainable_variables] = build_deep_exponential_family_variational()\n",
        "\n",
        "# @tf.function\n",
        "# def train_step(bag_of_words, step):\n",
        "#   with tf.GradientTape() as tape:\n",
        "#     # Compute expected log-likelihood. First, sample from the variational\n",
        "#     # distribution; second, compute the log-likelihood given the sample.\n",
        "#     qw2, qw1, qw0, qz2, qz1, qz0 = deep_exponential_family_variational()\n",
        "\n",
        "#     # Compute forward pass of model, setting value of the priors to the\n",
        "#     # approximate posterior samples. We also record the forward pass' execution\n",
        "#     # via ed.tape().\n",
        "#     with ed.tape() as model_tape:\n",
        "#       with ed.condition(w2=qw2, w1=qw1, w0=qw0,\n",
        "#                         z2=qz2, z1=qz1, z0=qz0):\n",
        "#         posterior_predictive = deep_exponential_family(data_size, feature_size, units, shape)\n",
        "\n",
        "#     log_likelihood = posterior_predictive.distribution.log_prob(bag_of_words)\n",
        "\n",
        "#     # Compute analytic KL-divergence between variational and prior distributions.\n",
        "#     kl = 0.\n",
        "#     for rv_name, variational_rv in [(\"z0\", qz0), (\"z1\", qz1), (\"z2\", qz2),\n",
        "#                                     (\"w0\", qw0), (\"w1\", qw1), (\"w2\", qw2)]:\n",
        "#       kl += tf.reduce_sum(variational_rv.distribution.kl_divergence(\n",
        "#           model_tape[rv_name].distribution))\n",
        "\n",
        "#     elbo = tf.reduce_mean(log_likelihood - kl)\n",
        "#     with writer.default():\n",
        "#       tf.summary.scalar(\"elbo\", elbo, step=step)\n",
        "#       loss = -elbo\n",
        "#   optimizer = tf.keras.optimizers.Adam(1e-3)\n",
        "#   gradients = tape.gradient(loss, trainable_variables)\n",
        "#   optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
        "#   return loss\n",
        "\n",
        "\n",
        "#   for step in range(max_steps):\n",
        "#     start_time = time.time()\n",
        "#     bag_of_words = next(train_data)\n",
        "#     loss = train_step(bag_of_words, step)\n",
        "#     if step % 500 == 0:\n",
        "#       writer.flush()\n",
        "#       duration = time.time() - start_time\n",
        "#       print(\"Step: {:>3d} Loss: {:.3f} ({:.3f} sec)\".format(\n",
        "#           step, elbo_value, duration))\n",
        "      \n",
        "# ter = train_step(bag_of_words, 0.01)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}